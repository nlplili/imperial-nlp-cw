{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_models_features.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nx7N7gY7MWng",
        "colab_type": "code",
        "outputId": "32451a9b-b4f1-4b8f-cedf-a779e3109093",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "from os.path import exists\n",
        "if not exists('ende_data.zip'):\n",
        "    !wget -O ende_data.zip https://competitions.codalab.org/my/datasets/download/c748d2c0-d6be-4e36-9f12-ca0e88819c4d\n",
        "    !unzip ende_data.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-26 21:11:20--  https://competitions.codalab.org/my/datasets/download/c748d2c0-d6be-4e36-9f12-ca0e88819c4d\n",
            "Resolving competitions.codalab.org (competitions.codalab.org)... 129.175.22.230\n",
            "Connecting to competitions.codalab.org (competitions.codalab.org)|129.175.22.230|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://newcodalab.lri.fr/prod-private/dataset_data_file/None/104ea/en-de.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=f6b01c2876a2eb7d72a2888a3348519ebe59845da5edf6861c5465c3c7579953&X-Amz-Date=20200226T211121Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20200226%2Fnewcodalab%2Fs3%2Faws4_request [following]\n",
            "--2020-02-26 21:11:21--  https://newcodalab.lri.fr/prod-private/dataset_data_file/None/104ea/en-de.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=f6b01c2876a2eb7d72a2888a3348519ebe59845da5edf6861c5465c3c7579953&X-Amz-Date=20200226T211121Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20200226%2Fnewcodalab%2Fs3%2Faws4_request\n",
            "Resolving newcodalab.lri.fr (newcodalab.lri.fr)... 129.175.15.11\n",
            "Connecting to newcodalab.lri.fr (newcodalab.lri.fr)|129.175.15.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 864010 (844K) [application/zip]\n",
            "Saving to: ‘ende_data.zip’\n",
            "\n",
            "ende_data.zip       100%[===================>] 843.76K  64.7KB/s    in 13s     \n",
            "\n",
            "2020-02-26 21:11:35 (65.3 KB/s) - ‘ende_data.zip’ saved [864010/864010]\n",
            "\n",
            "Archive:  ende_data.zip\n",
            "  inflating: dev.ende.mt             \n",
            "  inflating: dev.ende.scores         \n",
            "  inflating: dev.ende.src            \n",
            "  inflating: test.ende.mt            \n",
            "  inflating: test.ende.src           \n",
            "  inflating: train.ende.mt           \n",
            "  inflating: train.ende.scores       \n",
            "  inflating: train.ende.src          \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAz_War7Qy10",
        "colab_type": "code",
        "outputId": "abf10134-ba45-4157-ebec-38736c418ca2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if not exists('requirements.txt'):\n",
        "  !wget https://raw.githubusercontent.com/Unbabel/KiwiCutter/master/requirements.txt\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-26 16:18:25--  https://raw.githubusercontent.com/Unbabel/KiwiCutter/master/requirements.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1419 (1.4K) [text/plain]\n",
            "Saving to: ‘requirements.txt’\n",
            "\n",
            "\rrequirements.txt      0%[                    ]       0  --.-KB/s               \rrequirements.txt    100%[===================>]   1.39K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-02-26 16:18:25 (13.5 MB/s) - ‘requirements.txt’ saved [1419/1419]\n",
            "\n",
            "Collecting alembic==1.0.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/8b/0c98c378d93165d9809193f274c3c6e2151120d955b752419c7d43e4d857/alembic-1.0.11.tar.gz (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 2.8MB/s \n",
            "\u001b[?25hCollecting appnope==0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/87/a9/7985e6a53402f294c8f0e8eff3151a83f1fb901fa92909bb3ff29b4d22af/appnope-0.1.0-py2.py3-none-any.whl\n",
            "Collecting attrs==19.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/23/96/d828354fa2dbdf216eaa7b7de0db692f12c234f7ef888cc14980ef40d1d2/attrs-19.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: backcall==0.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (0.1.0)\n",
            "Requirement already satisfied: bleach==3.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (3.1.0)\n",
            "Collecting certifi==2019.6.16\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/1b/b853c7a9d4f6a6d00749e94eb6f3a041e342a885b87340b79c1ef73e3a78/certifi-2019.6.16-py2.py3-none-any.whl (157kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 18.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (3.0.4)\n",
            "Requirement already satisfied: Click==7.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (7.0)\n",
            "Collecting cloudpickle==1.2.1\n",
            "  Downloading https://files.pythonhosted.org/packages/09/f4/4a080c349c1680a2086196fcf0286a65931708156f39568ed7051e42ff6a/cloudpickle-1.2.1-py2.py3-none-any.whl\n",
            "Collecting colored==1.3.93\n",
            "  Downloading https://files.pythonhosted.org/packages/58/07/636616667b47d3115b0288311511c5fb446d0e499036b7db858704c89066/colored-1.3.93.tar.gz\n",
            "Collecting ConfigArgParse==0.14.0\n",
            "  Downloading https://files.pythonhosted.org/packages/55/ea/f0ade52790bcd687127a302b26c1663bf2e0f23210d5281dbfcd1dfcda28/ConfigArgParse-0.14.0.tar.gz\n",
            "Collecting configparser==3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/ab/1a/ec151e5e703ac80041eaccef923611bbcec2b667c20383655a06962732e9/configparser-3.8.1-py2.py3-none-any.whl\n",
            "Collecting databricks-cli==0.8.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5f/38/f83bc71c5e7351a03e8d44aaf04647d076bbf8f097e3f93b921704b7a74c/databricks_cli-0.8.7-py3-none-any.whl (82kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 8.3MB/s \n",
            "\u001b[?25hCollecting ddt==1.2.1\n",
            "  Downloading https://files.pythonhosted.org/packages/cf/f5/f83dea32dc3fb3be1e5afab8438dce73ed587740a2a061ae2ea56e04a36d/ddt-1.2.1-py2.py3-none-any.whl\n",
            "Collecting decorator==4.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/5f/88/0075e461560a1e750a0dcbf77f1d9de775028c37a19a346a6c565a257399/decorator-4.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: defusedxml==0.6.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 16)) (0.6.0)\n",
            "Collecting docker==4.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/47/5560c9cf0c92b50da24216f0e7733250fbed5a497f69e3c70e1be62143fe/docker-4.0.2-py2.py3-none-any.whl (138kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 16.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: entrypoints==0.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 18)) (0.3)\n",
            "Requirement already satisfied: Flask==1.1.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 19)) (1.1.1)\n",
            "Collecting gitdb2==2.0.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/30/a407568aa8d8f25db817cf50121a958722f3fc5f87e3a6fba1f40c0633e3/gitdb2-2.0.5-py2.py3-none-any.whl (62kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.9MB/s \n",
            "\u001b[?25hCollecting GitPython==3.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/c9/418ec7a9575f58c3670bab5caebc2cb28d6ac4cd6f52074bf75ab3ef2a28/GitPython-3.0.1-py3-none-any.whl (453kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 16.7MB/s \n",
            "\u001b[?25hCollecting gorilla==0.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e3/56/5a683944cbfc77e429c6f03c636ca50504a785f60ffae91ddd7f5f7bb520/gorilla-0.3.0-py2.py3-none-any.whl\n",
            "Collecting gunicorn==19.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/da/b8dd8deb741bff556db53902d4706774c8e1e67265f69528c14c003644e6/gunicorn-19.9.0-py2.py3-none-any.whl (112kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 22.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna==2.8 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 24)) (2.8)\n",
            "Collecting ipykernel==5.1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/16/43f51f65a8a08addf04f909a0938b06ba1ee1708b398a9282474531bd893/ipykernel-5.1.2-py3-none-any.whl (116kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 20.0MB/s \n",
            "\u001b[?25hCollecting ipython==7.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/c4/a79582814bdfe92bfca4d286a729304ffdf13f5135132cfcaea13cf1b2b3/ipython-7.7.0-py3-none-any.whl (774kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 19.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipython-genutils==0.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 27)) (0.2.0)\n",
            "Requirement already satisfied: ipywidgets==7.5.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 28)) (7.5.1)\n",
            "Requirement already satisfied: itsdangerous==1.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 29)) (1.1.0)\n",
            "Collecting jedi==0.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/55/54/da994f359e4e7da4776a200e76dbc85ba5fc319eefc22e33d55296d95a1d/jedi-0.15.1-py2.py3-none-any.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 23.7MB/s \n",
            "\u001b[?25hCollecting Jinja2==2.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/e7/fd8b501e7a6dfe492a433deb7b9d833d39ca74916fa8bc63dd1a4947a671/Jinja2-2.10.1-py2.py3-none-any.whl (124kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 32.1MB/s \n",
            "\u001b[?25hCollecting jsonschema==3.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/48/f5f11003ceddcd4ad292d4d9b5677588e9169eef41f88e38b2888e7ec6c4/jsonschema-3.0.2-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.2MB/s \n",
            "\u001b[?25hCollecting jupyter-client==5.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/4c/bf613864ae0644e2ac7d4a40bd209c40c8c71e3dc88d5f1d0aa92a68e716/jupyter_client-5.3.1-py2.py3-none-any.whl (91kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 9.8MB/s \n",
            "\u001b[?25hCollecting jupyter-core==4.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/25/6ffb0f6e57fa6ef5d2f814377133b361b42a6dd39105f4885a4f1666c2c3/jupyter_core-4.5.0-py2.py3-none-any.whl (78kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 7.2MB/s \n",
            "\u001b[?25hCollecting Mako==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/3c/8dcd6883d009f7cae0f3157fb53e9afb05a0d3d33b3db1268ec2e6f4a56b/Mako-1.1.0.tar.gz (463kB)\n",
            "\u001b[K     |████████████████████████████████| 471kB 27.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe==1.1.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 36)) (1.1.1)\n",
            "Requirement already satisfied: mistune==0.8.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 37)) (0.8.4)\n",
            "Collecting more-itertools==5.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/a6/42f17d065bda1fac255db13afc94c93dbfb64393eae37c749b4cb0752fc7/more_itertools-5.0.0-py3-none-any.whl (52kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.9MB/s \n",
            "\u001b[?25hCollecting nbconvert==5.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/df/4505c0a7fea624cac461d0f41051f33456ae656753f65cee8c2f43121cb2/nbconvert-5.6.0-py2.py3-none-any.whl (453kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 30.2MB/s \n",
            "\u001b[?25hCollecting nbformat==4.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/27/9a654d2b6cc1eaa517d1c5a4405166c7f6d72f04f6e7eea41855fe808a46/nbformat-4.4.0-py2.py3-none-any.whl (155kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 35.9MB/s \n",
            "\u001b[?25hCollecting nltk==3.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 40.5MB/s \n",
            "\u001b[?25hCollecting notebook==6.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/a1/1e07cedcb554408fefe4a7d32b2a041c86517167aec6ca8251c808ef6c1e/notebook-6.0.1-py3-none-any.whl (9.0MB)\n",
            "\u001b[K     |████████████████████████████████| 9.0MB 34.5MB/s \n",
            "\u001b[?25hCollecting numpy==1.17.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/b9/bda9781f0a74b90ebd2e046fde1196182900bd4a8e1ea503d3ffebc50e7c/numpy-1.17.0-cp36-cp36m-manylinux1_x86_64.whl (20.4MB)\n",
            "\u001b[K     |████████████████████████████████| 20.4MB 1.2MB/s \n",
            "\u001b[?25hCollecting openkiwi==0.1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/74/767e788ed15f4c7cf093d8013d81bfb86c7c9a09816bd557a52c7d948d46/openkiwi-0.1.2-py3-none-any.whl (174kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 47.2MB/s \n",
            "\u001b[?25hCollecting pandas==0.25.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/9a/7eb9952f4b4d73fbd75ad1d5d6112f407e695957444cb695cbb3cdab918a/pandas-0.25.0-cp36-cp36m-manylinux1_x86_64.whl (10.5MB)\n",
            "\u001b[K     |████████████████████████████████| 10.5MB 45.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandocfilters==1.4.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 46)) (1.4.2)\n",
            "Collecting parso==0.5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/bd/bf4e5bd01d79906e5b945a7af033154da49fd2b0d5b5c705a21330323305/parso-0.5.1-py2.py3-none-any.whl (95kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 11.3MB/s \n",
            "\u001b[?25hCollecting pexpect==4.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/3e/377007e3f36ec42f1b84ec322ee12141a9e10d808312e5738f52f80a232c/pexpect-4.7.0-py2.py3-none-any.whl (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: pickleshare==0.7.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 49)) (0.7.5)\n",
            "Requirement already satisfied: prometheus-client==0.7.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 50)) (0.7.1)\n",
            "Collecting prompt-toolkit==2.0.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/a7/9b1dd14ef45345f186ef69d175bdd2491c40ab1dfa4b2b3e4352df719ed7/prompt_toolkit-2.0.9-py3-none-any.whl (337kB)\n",
            "\u001b[K     |████████████████████████████████| 337kB 47.7MB/s \n",
            "\u001b[?25hCollecting protobuf==3.9.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/f4/a27952733796330cd17c17ea1f974459f5fefbbad119c0f296a6d807fec3/protobuf-3.9.1-cp36-cp36m-manylinux1_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 43.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: ptyprocess==0.6.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 53)) (0.6.0)\n",
            "Collecting Pygments==2.4.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/73/1dfa428150e3ccb0fa3e68db406e5be48698f2a979ccbcec795f28f44048/Pygments-2.4.2-py2.py3-none-any.whl (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 37.8MB/s \n",
            "\u001b[?25hCollecting pyrsistent==0.15.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/66/b2638d96a2d128b168d0dba60fdc77b7800a9b4a5340cefcc5fc4eae6295/pyrsistent-0.15.4.tar.gz (107kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 50.7MB/s \n",
            "\u001b[?25hCollecting python-dateutil==2.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/17/c62faccbfbd163c7f57f3844689e3a78bae1f403648a6afb1d0866d87fbb/python_dateutil-2.8.0-py2.py3-none-any.whl (226kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 44.5MB/s \n",
            "\u001b[?25hCollecting python-editor==1.0.4\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\n",
            "Collecting pytz==2019.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/76/46d697698a143e05f77bec5a526bf4e56a0be61d63425b68f4ba553b51f2/pytz-2019.2-py2.py3-none-any.whl (508kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 37.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML==3.13 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 59)) (3.13)\n",
            "Collecting pyzmq==18.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/89/6f0ea51ffa9c2c00c0ab0460f137b16a5ab5b47e3b060c5b1fc9ca425836/pyzmq-18.1.0-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 43.8MB/s \n",
            "\u001b[?25hCollecting querystring-parser==1.2.4\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/fa/f54f5662e0eababf0c49e92fd94bf178888562c0e7b677c8941bbbcd1bd6/querystring_parser-1.2.4.tar.gz\n",
            "Collecting requests==2.22.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.8MB/s \n",
            "\u001b[?25hCollecting scipy==1.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/50/a552a5aff252ae915f522e44642bb49a7b7b31677f9580cfd11bcc869976/scipy-1.3.1-cp36-cp36m-manylinux1_x86_64.whl (25.2MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: Send2Trash==1.5.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 64)) (1.5.0)\n",
            "Collecting simplejson==3.16.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/24/c35fb1c1c315fc0fffe61ea00d3f88e85469004713dab488dee4f35b0aff/simplejson-3.16.0.tar.gz (81kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: six==1.12.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 66)) (1.12.0)\n",
            "Collecting smmap2==2.0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/55/d2/866d45e3a121ee15a1dc013824d58072fd5c7799c9c34d01378eb262ca8f/smmap2-2.0.5-py2.py3-none-any.whl\n",
            "Collecting SQLAlchemy==1.3.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c2/75/6217c626fa22ad56ae5ccb1a36e7c4f17f5ca31543887e00179468d10464/SQLAlchemy-1.3.7.tar.gz (5.9MB)\n",
            "\u001b[K     |████████████████████████████████| 5.9MB 41.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlparse==0.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 69)) (0.3.0)\n",
            "Collecting tabulate==0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c2/fd/202954b3f0eb896c53b7b6f07390851b1fd2ca84aa95880d7ae4f434c4ac/tabulate-0.8.3.tar.gz (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.8MB/s \n",
            "\u001b[?25hCollecting terminado==0.8.2\n",
            "  Downloading https://files.pythonhosted.org/packages/a7/56/80ea7fa66565fa75ae21ce0c16bc90067530e5d15e48854afcc86585a391/terminado-0.8.2-py2.py3-none-any.whl\n",
            "Collecting testpath==0.4.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/be/a4/162f9ebb6489421fe46dcca2ae420369edfee4b563c668d93cb4605d12ba/testpath-0.4.2-py2.py3-none-any.whl (163kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 45.0MB/s \n",
            "\u001b[?25hCollecting torch==1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/57/d5cceb0799c06733eefce80c395459f28970ebb9e896846ce96ab579a3f1/torch-1.2.0-cp36-cp36m-manylinux1_x86_64.whl (748.8MB)\n",
            "\u001b[K     |████████████████████████████████| 748.9MB 20kB/s \n",
            "\u001b[?25hRequirement already satisfied: torchtext==0.3.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 74)) (0.3.1)\n",
            "Collecting tornado==6.0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/78/2d2823598496127b21423baffaa186b668f73cd91887fcef78b6eade136b/tornado-6.0.3.tar.gz (482kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 39.7MB/s \n",
            "\u001b[?25hCollecting tqdm==4.34.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/83/06029af22fe06b8a7be013aeae5e104b3ed26867e5d4ca91408b30aa602e/tqdm-4.34.0-py2.py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.4MB/s \n",
            "\u001b[?25hCollecting traitlets==4.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/d6/abcb22de61d78e2fc3959c964628a5771e47e7cc60d53e9342e21ed6cc9a/traitlets-4.3.2-py2.py3-none-any.whl (74kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.4MB/s \n",
            "\u001b[?25hCollecting urllib3==1.25.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/60/247f23a7121ae632d62811ba7f273d0e58972d75e58a94d329d51550a47d/urllib3-1.25.3-py2.py3-none-any.whl (150kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 46.1MB/s \n",
            "\u001b[?25hCollecting wcwidth==0.1.7\n",
            "  Downloading https://files.pythonhosted.org/packages/7e/9f/526a6947247599b084ee5232e4f9190a38f398d7300d866af3ab571a5bfe/wcwidth-0.1.7-py2.py3-none-any.whl\n",
            "Requirement already satisfied: webencodings==0.5.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 80)) (0.5.1)\n",
            "Collecting websocket-client==0.56.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/19/44753eab1fdb50770ac69605527e8859468f3c0fd7dc5a76dd9c4dbd7906/websocket_client-0.56.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 37.8MB/s \n",
            "\u001b[?25hCollecting Werkzeug==0.15.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/ab/d3bed6b92042622d24decc7aadc8877badf18aeca1571045840ad4956d3f/Werkzeug-0.15.5-py2.py3-none-any.whl (328kB)\n",
            "\u001b[K     |████████████████████████████████| 337kB 55.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: widgetsnbextension==3.5.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 83)) (3.5.1)\n",
            "Collecting yamlmagic==0.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/bd/db/d677d565b9048b9003ae6aac3ec34cce9dcc0e9c13bd68289c7c8dde3959/yamlmagic-0.2.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython==7.7.0->-r requirements.txt (line 26)) (45.1.0)\n",
            "Building wheels for collected packages: alembic, colored, ConfigArgParse, Mako, nltk, pyrsistent, querystring-parser, simplejson, SQLAlchemy, tabulate, tornado\n",
            "  Building wheel for alembic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for alembic: filename=alembic-1.0.11-py2.py3-none-any.whl size=162176 sha256=fc7a01de9981bcf38135b8005399173ab201566c72b37b5a8f8e6d758aa2da32\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/65/b2/9837b4422d13e739c3324c428f1b3aa9e3c3df666bb420e4b3\n",
            "  Building wheel for colored (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for colored: filename=colored-1.3.93-cp36-none-any.whl size=12575 sha256=89453bd039e5762499771decafb3618ede98a8b582eef55ecb8e08a0b0af2c98\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/c3/b7/ac21460710230feb409fee89bf594c4f2660ff7b67491d128f\n",
            "  Building wheel for ConfigArgParse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ConfigArgParse: filename=ConfigArgParse-0.14.0-cp36-none-any.whl size=17504 sha256=2c9c5e7e0eca8d2f03dc7a22017e501f42b6624eb24d53abeea88884cc50c2f8\n",
            "  Stored in directory: /root/.cache/pip/wheels/aa/9c/ce/7e904dddb8c7595ffbe3409d24455bc5005852850e36011bda\n",
            "  Building wheel for Mako (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Mako: filename=Mako-1.1.0-cp36-none-any.whl size=75362 sha256=44fffe27ad4eb7acb2f2bef9c949f71a6417580afe36ef4c4598c7ba8c3effe5\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/32/7b/a291926643fc1d1e02593e0d9e247c5a866a366b8343b7aa27\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-cp36-none-any.whl size=1449907 sha256=8d1aac5e7a79383d1f1b97106e4728a051ece1004bbecdcd6b0830b795174b63\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
            "  Building wheel for pyrsistent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyrsistent: filename=pyrsistent-0.15.4-cp36-cp36m-linux_x86_64.whl size=97559 sha256=43ce16acbd4d695116f16c1e44f0838d88551ee37b97cb1aee142c5aa5068a32\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/46/00/6d471ef0b813e3621f0abe6cb723c20d529d39a061de3f7c51\n",
            "  Building wheel for querystring-parser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for querystring-parser: filename=querystring_parser-1.2.4-cp36-none-any.whl size=7079 sha256=f278624c2039d5934fe03bf572ca0d91a41abb5d5893321bffe06b5c660e901b\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/41/34/23ebf5d1089a9aed847951e0ee375426eb4ad0a7079d88d41e\n",
            "  Building wheel for simplejson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for simplejson: filename=simplejson-3.16.0-cp36-cp36m-linux_x86_64.whl size=114016 sha256=94d6bf68c4e5f86d87c78c92ea27de89441490654235abdd26c711a2415c54a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/5d/1a/1e/0350bb3df3e74215cd91325344cc86c2c691f5306eb4d22c77\n",
            "  Building wheel for SQLAlchemy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for SQLAlchemy: filename=SQLAlchemy-1.3.7-cp36-cp36m-linux_x86_64.whl size=1191598 sha256=7159b5ee3db812bb0d329b00337e1ce7b697ee78b39d99fdbbae504812f2d5df\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/e0/68/3d11cc7209b5bd2c7d55cbb56c6bda843cc82f77c8387468ea\n",
            "  Building wheel for tabulate (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tabulate: filename=tabulate-0.8.3-cp36-none-any.whl size=23379 sha256=975ed57f79f4b36c02a58121799faaa936714ce4d6ca29f9f8eca86712123ded\n",
            "  Stored in directory: /root/.cache/pip/wheels/2b/67/89/414471314a2d15de625d184d8be6d38a03ae1e983dbda91e84\n",
            "  Building wheel for tornado (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tornado: filename=tornado-6.0.3-cp36-cp36m-linux_x86_64.whl size=423199 sha256=9dbc401df96bfe4a5b6e1b69d580f16e65362a090742db9ff8ffcc04105f3fba\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/bf/40/2f6ef700f48401ca40e5e3dd7d0e3c0a90e064897b7fe5fc08\n",
            "Successfully built alembic colored ConfigArgParse Mako nltk pyrsistent querystring-parser simplejson SQLAlchemy tabulate tornado\n",
            "\u001b[31mERROR: torchvision 0.5.0 has requirement torch==1.4.0, but you'll have torch 1.2.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: kaggle 1.5.6 has requirement urllib3<1.25,>=1.21.1, but you'll have urllib3 1.25.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: jupyter-console 5.2.0 has requirement prompt-toolkit<2.0.0,>=1.0.0, but you'll have prompt-toolkit 2.0.9 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement ipykernel~=4.6.0, but you'll have ipykernel 5.1.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement ipython~=5.5.0, but you'll have ipython 7.7.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement notebook~=5.2.0, but you'll have notebook 6.0.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.21.0, but you'll have requests 2.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement tornado~=4.5.0, but you'll have tornado 6.0.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: SQLAlchemy, Mako, python-editor, python-dateutil, alembic, appnope, attrs, certifi, cloudpickle, colored, ConfigArgParse, configparser, tabulate, urllib3, requests, databricks-cli, ddt, decorator, websocket-client, docker, smmap2, gitdb2, GitPython, gorilla, gunicorn, traitlets, tornado, pyzmq, jupyter-core, jupyter-client, Pygments, pexpect, wcwidth, prompt-toolkit, parso, jedi, ipython, ipykernel, Jinja2, pyrsistent, jsonschema, more-itertools, testpath, nbformat, nbconvert, nltk, terminado, notebook, numpy, tqdm, torch, scipy, openkiwi, pytz, pandas, protobuf, querystring-parser, simplejson, Werkzeug, yamlmagic\n",
            "  Found existing installation: SQLAlchemy 1.3.13\n",
            "    Uninstalling SQLAlchemy-1.3.13:\n",
            "      Successfully uninstalled SQLAlchemy-1.3.13\n",
            "  Found existing installation: python-dateutil 2.6.1\n",
            "    Uninstalling python-dateutil-2.6.1:\n",
            "      Successfully uninstalled python-dateutil-2.6.1\n",
            "  Found existing installation: attrs 19.3.0\n",
            "    Uninstalling attrs-19.3.0:\n",
            "      Successfully uninstalled attrs-19.3.0\n",
            "  Found existing installation: certifi 2019.11.28\n",
            "    Uninstalling certifi-2019.11.28:\n",
            "      Successfully uninstalled certifi-2019.11.28\n",
            "  Found existing installation: cloudpickle 1.2.2\n",
            "    Uninstalling cloudpickle-1.2.2:\n",
            "      Successfully uninstalled cloudpickle-1.2.2\n",
            "  Found existing installation: tabulate 0.8.6\n",
            "    Uninstalling tabulate-0.8.6:\n",
            "      Successfully uninstalled tabulate-0.8.6\n",
            "  Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Found existing installation: requests 2.21.0\n",
            "    Uninstalling requests-2.21.0:\n",
            "      Successfully uninstalled requests-2.21.0\n",
            "  Found existing installation: decorator 4.4.1\n",
            "    Uninstalling decorator-4.4.1:\n",
            "      Successfully uninstalled decorator-4.4.1\n",
            "  Found existing installation: gunicorn 20.0.4\n",
            "    Uninstalling gunicorn-20.0.4:\n",
            "      Successfully uninstalled gunicorn-20.0.4\n",
            "  Found existing installation: traitlets 4.3.3\n",
            "    Uninstalling traitlets-4.3.3:\n",
            "      Successfully uninstalled traitlets-4.3.3\n",
            "  Found existing installation: tornado 4.5.3\n",
            "    Uninstalling tornado-4.5.3:\n",
            "      Successfully uninstalled tornado-4.5.3\n",
            "  Found existing installation: pyzmq 17.0.0\n",
            "    Uninstalling pyzmq-17.0.0:\n",
            "      Successfully uninstalled pyzmq-17.0.0\n",
            "  Found existing installation: jupyter-core 4.6.2\n",
            "    Uninstalling jupyter-core-4.6.2:\n",
            "      Successfully uninstalled jupyter-core-4.6.2\n",
            "  Found existing installation: jupyter-client 5.3.4\n",
            "    Uninstalling jupyter-client-5.3.4:\n",
            "      Successfully uninstalled jupyter-client-5.3.4\n",
            "  Found existing installation: Pygments 2.1.3\n",
            "    Uninstalling Pygments-2.1.3:\n",
            "      Successfully uninstalled Pygments-2.1.3\n",
            "  Found existing installation: pexpect 4.8.0\n",
            "    Uninstalling pexpect-4.8.0:\n",
            "      Successfully uninstalled pexpect-4.8.0\n",
            "  Found existing installation: wcwidth 0.1.8\n",
            "    Uninstalling wcwidth-0.1.8:\n",
            "      Successfully uninstalled wcwidth-0.1.8\n",
            "  Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Found existing installation: parso 0.6.1\n",
            "    Uninstalling parso-0.6.1:\n",
            "      Successfully uninstalled parso-0.6.1\n",
            "  Found existing installation: jedi 0.16.0\n",
            "    Uninstalling jedi-0.16.0:\n",
            "      Successfully uninstalled jedi-0.16.0\n",
            "  Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "  Found existing installation: ipykernel 4.6.1\n",
            "    Uninstalling ipykernel-4.6.1:\n",
            "      Successfully uninstalled ipykernel-4.6.1\n",
            "  Found existing installation: Jinja2 2.11.1\n",
            "    Uninstalling Jinja2-2.11.1:\n",
            "      Successfully uninstalled Jinja2-2.11.1\n",
            "  Found existing installation: pyrsistent 0.15.7\n",
            "    Uninstalling pyrsistent-0.15.7:\n",
            "      Successfully uninstalled pyrsistent-0.15.7\n",
            "  Found existing installation: jsonschema 2.6.0\n",
            "    Uninstalling jsonschema-2.6.0:\n",
            "      Successfully uninstalled jsonschema-2.6.0\n",
            "  Found existing installation: more-itertools 8.2.0\n",
            "    Uninstalling more-itertools-8.2.0:\n",
            "      Successfully uninstalled more-itertools-8.2.0\n",
            "  Found existing installation: testpath 0.4.4\n",
            "    Uninstalling testpath-0.4.4:\n",
            "      Successfully uninstalled testpath-0.4.4\n",
            "  Found existing installation: nbformat 5.0.4\n",
            "    Uninstalling nbformat-5.0.4:\n",
            "      Successfully uninstalled nbformat-5.0.4\n",
            "  Found existing installation: nbconvert 5.6.1\n",
            "    Uninstalling nbconvert-5.6.1:\n",
            "      Successfully uninstalled nbconvert-5.6.1\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "  Found existing installation: terminado 0.8.3\n",
            "    Uninstalling terminado-0.8.3:\n",
            "      Successfully uninstalled terminado-0.8.3\n",
            "  Found existing installation: notebook 5.2.2\n",
            "    Uninstalling notebook-5.2.2:\n",
            "      Successfully uninstalled notebook-5.2.2\n",
            "  Found existing installation: numpy 1.17.5\n",
            "    Uninstalling numpy-1.17.5:\n",
            "      Successfully uninstalled numpy-1.17.5\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "  Found existing installation: torch 1.4.0\n",
            "    Uninstalling torch-1.4.0:\n",
            "      Successfully uninstalled torch-1.4.0\n",
            "  Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Found existing installation: pytz 2018.9\n",
            "    Uninstalling pytz-2018.9:\n",
            "      Successfully uninstalled pytz-2018.9\n",
            "  Found existing installation: pandas 0.25.3\n",
            "    Uninstalling pandas-0.25.3:\n",
            "      Successfully uninstalled pandas-0.25.3\n",
            "  Found existing installation: protobuf 3.10.0\n",
            "    Uninstalling protobuf-3.10.0:\n",
            "      Successfully uninstalled protobuf-3.10.0\n",
            "  Found existing installation: Werkzeug 1.0.0\n",
            "    Uninstalling Werkzeug-1.0.0:\n",
            "      Successfully uninstalled Werkzeug-1.0.0\n",
            "Successfully installed ConfigArgParse-0.14.0 GitPython-3.0.1 Jinja2-2.10.1 Mako-1.1.0 Pygments-2.4.2 SQLAlchemy-1.3.7 Werkzeug-0.15.5 alembic-1.0.11 appnope-0.1.0 attrs-19.1.0 certifi-2019.6.16 cloudpickle-1.2.1 colored-1.3.93 configparser-3.8.1 databricks-cli-0.8.7 ddt-1.2.1 decorator-4.4.0 docker-4.0.2 gitdb2-2.0.5 gorilla-0.3.0 gunicorn-19.9.0 ipykernel-5.1.2 ipython-7.7.0 jedi-0.15.1 jsonschema-3.0.2 jupyter-client-5.3.1 jupyter-core-4.5.0 more-itertools-5.0.0 nbconvert-5.6.0 nbformat-4.4.0 nltk-3.4.5 notebook-6.0.1 numpy-1.17.0 openkiwi-0.1.2 pandas-0.25.0 parso-0.5.1 pexpect-4.7.0 prompt-toolkit-2.0.9 protobuf-3.9.1 pyrsistent-0.15.4 python-dateutil-2.8.0 python-editor-1.0.4 pytz-2019.2 pyzmq-18.1.0 querystring-parser-1.2.4 requests-2.22.0 scipy-1.3.1 simplejson-3.16.0 smmap2-2.0.5 tabulate-0.8.3 terminado-0.8.2 testpath-0.4.2 torch-1.2.0 tornado-6.0.3 tqdm-4.34.0 traitlets-4.3.2 urllib3-1.25.3 wcwidth-0.1.7 websocket-client-0.56.0 yamlmagic-0.2.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "certifi",
                  "cloudpickle",
                  "dateutil",
                  "decorator",
                  "google",
                  "ipykernel",
                  "jupyter_client",
                  "jupyter_core",
                  "numpy",
                  "pandas",
                  "pexpect",
                  "prompt_toolkit",
                  "pygments",
                  "pytz",
                  "requests",
                  "tornado",
                  "tqdm",
                  "traitlets",
                  "urllib3",
                  "wcwidth",
                  "zmq"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMwGu7R0Y33T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# All imports\n",
        "# !git clone https://github.com/facebookresearch/fastText.git\n",
        "# !pip install ./fastText/.\n",
        "# import fasttext\n",
        "# import fasttext.util\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "from nltk import download\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity as sim\n",
        "\n",
        "import pickle\n",
        "\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "import tqdm\n",
        "\n",
        "# import kiwi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8aHBVbcSKQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get OpenKiwi model\n",
        "if not exists('estimator_en_de.zip'):\n",
        "    !wget -o estimator_en_de.zip 'https://github.com/unbabel/KiwiCutter/releases/download/v1.0/estimator_en_de.torch.zip'\n",
        "    !unzip estimator_en_de.torch\n",
        "model_kiwi = kiwi.load_model('estimator_en_de.torch')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gZWbXa0Wz8w",
        "colab_type": "code",
        "outputId": "f0e1fd32-64cf-4862-e131-12a22b1f0feb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model_kiwi.predict({'source': ['I like to hike in the mountains'],\n",
        "                    'target':['Ich wandere gerne in den Bergen']})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'tags': [[0.9378803372383118,\n",
              "   0.9655714631080627,\n",
              "   0.9754980206489563,\n",
              "   0.9426648616790771,\n",
              "   0.9505487680435181,\n",
              "   0.9422469735145569]],\n",
              " 'gap_tags': [[0.5304547548294067,\n",
              "   0.06496486812829971,\n",
              "   0.0730169266462326,\n",
              "   0.08218620717525482,\n",
              "   0.037235017865896225,\n",
              "   0.15140244364738464,\n",
              "   0.025262480601668358]],\n",
              " 'sentence_scores': [0.646120548248291]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxB92Q8LotmW",
        "colab_type": "text"
      },
      "source": [
        "# **Preprocessing and Feature Extraction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucSsW-VrzluM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Embedding:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.ft = None\n",
        "    self.ft_de = None\n",
        "    self.nlp_de = None\n",
        "    self.nlp_en = None\n",
        "    self.wvecs = None\n",
        "    self.german_wvecs = None\n",
        "    download('stopwords') #stopwords dictionary, run once\n",
        "    self.stop_words_en = set(stopwords.words('english'))\n",
        "    self.stop_words_de = set(stopwords.words('german'))\n",
        "\n",
        "  def download_fast_text(self):\n",
        "    !wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
        "    !gunzip cc.en.300.bin.gz\n",
        "    !wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.de.300.bin.gz\n",
        "    !gunzip cc.de.300.bin.gz\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    self.ft = fasttext.load_model('cc.en.300.bin')\n",
        "    self.ft_de = fasttext.load_model('cc.de.300.bin')\n",
        "    fasttext.util.reduce_model(self.ft, 100)\n",
        "    self.ft.save_model('/drive/My Drive/cc.en.100.bin')\n",
        "\n",
        "    fasttext.util.reduce_model(self.ft_de, 100)\n",
        "    self.ft.save_model('drive/My Drive/cc.de.100.bin')\n",
        "\n",
        "  def load_fast_text(self):\n",
        "    self.ft_en = fasttext.load_model('drive/My Drive/cc.de.100.bin')\n",
        "    self.ft_de = fasttext.load_model('drive/My Drive/cc.en.100.bin')\n",
        "\n",
        "  def load_spacy(self):\n",
        "    !spacy download en_core_web_md\n",
        "    !spacy link en_core_web_md en300\n",
        "\n",
        "    !spacy download de_core_news_md\n",
        "    !spacy link de_core_news_md de300\n",
        "\n",
        "    self.nlp_de = spacy.load('de300')\n",
        "    self.nlp_en = spacy.load('en300')\n",
        "\n",
        "  def load_muse(self):\n",
        "    !wget https://dl.fbaipublicfiles.com/arrival/vectors/wiki.multi.en.vec\n",
        "    !wget https://dl.fbaipublicfiles.com/arrival/vectors/wiki.multi.de.vec\n",
        "\n",
        "    self.wvecs = {}\n",
        "    with open(\"./wiki.multi.en.vec\", \"r\") as ende_src:\n",
        "      for line in ende_src:\n",
        "        word = line.split(\" \")[0]\n",
        "        vector = [float(a) for a in line.split(\" \")[1:]]\n",
        "        self.wvecs[word] = vector\n",
        "\n",
        "    self.german_wvecs = {}\n",
        "    with open(\"./wiki.multi.de.vec\", \"r\") as ende_src:\n",
        "      for line in ende_src:\n",
        "        word = line.split(\" \")[0]\n",
        "        vector = [float(a) for a in line.split(\" \")[1:]]\n",
        "        self.german_wvecs[word] = vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgSp6q5CyzN1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Helper functions\n",
        "def get_sentence_emb(line, nlp, lang):\n",
        "  if lang == 'en':\n",
        "    text = line.lower()\n",
        "    l = [token.lemma_ for token in nlp.tokenizer(text)]\n",
        "    l = ' '.join([word for word in l if word not in embedding.stop_words_en])\n",
        "\n",
        "  elif lang == 'de':\n",
        "    text = line.lower()\n",
        "    l = [token.lemma_ for token in nlp.tokenizer(text)]\n",
        "    l = ' '.join([word for word in l if word not in embedding.stop_words_de])\n",
        "\n",
        "  sentence = nlp(l)\n",
        "  return sentence.vector\n",
        "\n",
        "def get_sentence_emb_using_word_embs(line, nlp, lang):\n",
        "    if lang == 'en':\n",
        "        text = line.lower()\n",
        "        l = [token.lemma_ for token in nlp.tokenizer(text)]\n",
        "        l = [word for word in l if word not in embedding.stop_words_en]\n",
        "\n",
        "    elif lang == 'de':\n",
        "        text = line.lower()\n",
        "        l = [token.lemma_ for token in nlp.tokenizer(text)]\n",
        "        l = [word for word in l if word not in embedding.stop_words_de]\n",
        "\n",
        "    sentence = []\n",
        "    for word in l:\n",
        "        sentence.append(nlp(word).vector)\n",
        "    return sentence\n",
        "\n",
        "def get_tokens(line, lang, nlp):\n",
        "  text = line.lower()\n",
        "  l = [token.lemma_ for token in nlp.tokenizer(text)]\n",
        "  l = ' '.join([word for word in l])\n",
        "  return nlp(l)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qdGPIRTdjO3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FeatureExtraction:\n",
        "  @staticmethod\n",
        "  def get_pos_tag_counts(line, lang, nlp):\n",
        "    pos_counts = {'ADJ': 0,\n",
        "                  'ADP': 0,\n",
        "                  'ADV': 0,\n",
        "                  'AUX': 0,\n",
        "                  'CONJ':\t0,\n",
        "                  'CCONJ':0,\n",
        "                  'DET':\t0,\n",
        "                  'INTJ':\t0,\n",
        "                  'NOUN': 0,\n",
        "                  'NUM': 0,\n",
        "                  'PART':\t0,\n",
        "                  'PRON': 0,\n",
        "                  'PROPN': 0,\n",
        "                  'PUNCT': 0,\n",
        "                  'SCONJ': 0,\n",
        "                  'SYM': 0,\n",
        "                  'VERB': 0,\n",
        "                  'X': 0,\n",
        "                  'SPACE': 0}\n",
        "    \n",
        "    sen = get_tokens(line, lang, nlp)\n",
        "    for token in sen:\n",
        "      pos_counts[token.pos_] += 1\n",
        "    \n",
        "    return pos_counts\n",
        "\n",
        "  @staticmethod\n",
        "  def get_kiwi_scores(en_sent, de_sent):\n",
        "    scores = model_kiwi.predict({'source': [en_sent], 'target':[de_sent]})\n",
        "    return scores['sentence_scores'][0]\n",
        "\n",
        "  @staticmethod\n",
        "  def get_cosine_sim_with_golden_translations(filepath):\n",
        "    file = open(filepath)\n",
        "    lines = file.readlines()\n",
        "    return np.array(lines).astype(float)\n",
        "\n",
        "  @staticmethod\n",
        "  def get_named_entities(line, lang, nlp):\n",
        "    named_entities = {\n",
        "        u'CARDINAL': 0, \n",
        "        u'DATE': 0, \n",
        "        u'EVENT': 0, \n",
        "        u'FAC': 0, \n",
        "        u'GPE': 0, \n",
        "        u'LANGUAGE': 0, \n",
        "        u'LAW': 0, \n",
        "        u'LOC': 0, \n",
        "        u'MONEY': 0, \n",
        "        u'NORP': 0, \n",
        "        u'ORDINAL': 0, \n",
        "        u'ORG': 0, \n",
        "        u'PERCENT': 0, \n",
        "        u'PERSON': 0, \n",
        "        u'PRODUCT': 0, \n",
        "        u'QUANTITY': 0, \n",
        "        u'TIME': 0, \n",
        "        u'WORK_OF_ART': 0, \n",
        "        u'': 0, \n",
        "        u'MISC': 0,\n",
        "        u'PER': 0\n",
        "    }\n",
        "    sen = get_tokens(line, lang, nlp)\n",
        "    for ent in sen.ents:\n",
        "      if ent.label_ in named_entities:\n",
        "        named_entities[ent.label_] += 1\n",
        "      else:\n",
        "        print(\"Encountered unknown label:\", ent.label_)\n",
        "        named_entities[''] += 1\n",
        "    \n",
        "    return named_entities\n",
        "\n",
        "  @staticmethod\n",
        "  def get_num_of_tokens(sent, lang, nlp):\n",
        "    tokens = get_tokens(sent, lang, nlp)\n",
        "    return len(tokens)\n",
        "\n",
        "  @staticmethod\n",
        "  def get_avg_source_token_length(sent, nlp_en):\n",
        "    tokens = get_tokens(sent, 'en', nlp_en)\n",
        "    sum = 0\n",
        "    for token in tokens:\n",
        "      sum += len(token)\n",
        "    return sum / len(tokens)\n",
        "\n",
        "  @staticmethod\n",
        "  def get_num_of_punctuations(sent, nlp):\n",
        "    punctuation = [',','.','...','\\'', '\"', '(', ')', '[', ']']\n",
        "    tokens = get_tokens(sent, 'en', nlp)\n",
        "    sum = 0\n",
        "    for token in tokens:\n",
        "      if token.text in punctuation:\n",
        "        sum += 1\n",
        "    return sum\n",
        "\n",
        "  @staticmethod\n",
        "  def get_num_of_numberic_tokens(sent, nlp):\n",
        "    tokens = get_tokens(sent, 'en', nlp)\n",
        "    sum = 0\n",
        "    for token in tokens:\n",
        "      if token.text.isdigit():\n",
        "        sum += 1\n",
        "    return sum\n",
        "\n",
        "  @staticmethod\n",
        "  def get_num_of_alpha_tokens(sent, nlp):\n",
        "    tokens = get_tokens(sent, 'en', nlp)\n",
        "    sum = 0\n",
        "    for token in tokens:\n",
        "      if token.text.isalpha():\n",
        "        sum += 1\n",
        "    return sum\n",
        "\n",
        "  @staticmethod\n",
        "  def get_ratio_of_target_source_lengths(en_sent, de_sent, nlp_en, nlp_de):\n",
        "    standard_ratio_of_target_source = 219 / 200\n",
        "    num_of_tokens_en = FeatureExtraction.get_num_of_tokens(en_sent, 'en', nlp_en)\n",
        "    num_of_tokens_de = FeatureExtraction.get_num_of_tokens(de_sent, 'de', nlp_de)\n",
        "    return (num_of_tokens_en / num_of_tokens_de) - standard_ratio_of_target_source\n",
        "\n",
        "  @staticmethod\n",
        "  def get_percentage_of_numbers_in_target(sent, nlp):\n",
        "    return FeatureExtraction.get_num_of_numberic_tokens(sent, nlp) / FeatureExtraction.get_num_of_tokens(sent, 'de', nlp)\n",
        "\n",
        "  # Helper method\n",
        "  @staticmethod\n",
        "  def get_most_matching_words(matrix):\n",
        "    row_ind, col_ind = linear_sum_assignment(matrix)\n",
        "    return row_ind, col_ind\n",
        "\n",
        "  # Helper method\n",
        "  @staticmethod\n",
        "  def find_order(arr):\n",
        "    row_ind, col_ind = get_most_matching_words(arr)\n",
        "    return col_ind\n",
        "\n",
        "  @staticmethod\n",
        "  def get_similarities(english_embs, german_embs):\n",
        "    x_vals = []\n",
        "    for idx in tqdm.tqdm(range(len(german_embs))):\n",
        "      arr = None\n",
        "      for i in range(len(german_embs[idx])):\n",
        "        inner_arr = []\n",
        "        for j in range(len(english_embs[idx])):\n",
        "          inner_arr.append(-sim([german_embs[idx][i],english_embs[idx][j]])[0][1])\n",
        "        if arr is None:\n",
        "          arr = np.array([inner_arr])\n",
        "        else:\n",
        "          arr = np.concatenate((arr, [inner_arr]), axis=0)\n",
        "      if arr is None:\n",
        "        x_vals.append([])\n",
        "        continue\n",
        "      max_length = max(len(german_embs[idx]), len(english_embs[idx]))\n",
        "      blanks = np.zeros((max_length,max_length))\n",
        "      blanks[:arr.shape[0],:arr.shape[1]] = arr\n",
        "      arr = blanks\n",
        "      order = find_order(arr)\n",
        "      vals = []\n",
        "      for i in range(len(order)):\n",
        "        vals.append(arr[i][order[i]])\n",
        "      x_vals.append(np.array(vals))\n",
        "      if idx % 100 == 0:\n",
        "        print(idx)\n",
        "    return x_vals"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoJ6kp6JWxM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Preprocessing:\n",
        "\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def pad_sentences(self, embeddings):\n",
        "    pad = 2900\n",
        "    padded_embeddings = []\n",
        "    for i in embeddings:\n",
        "      padded_embeddings.append(np.concatenate((i, ([0] * (pad - len(i)))), axis=0))\n",
        "    return padded_embeddings\n",
        "\n",
        "  def get_fast_text_embeddings(self, f, nlp, stopwords, ftm):\n",
        "    punctuation = [',','.','...','\\'', '\"', '(', ')', '[', ']']\n",
        "    lines_embs = []\n",
        "    file = open(f) \n",
        "    lines = file.readlines()\n",
        "    for l in lines:\n",
        "      text = l.lower()\n",
        "      l = [token.lemma_ for token in nlp.tokenizer(text)]\n",
        "      l = [word for word in l if word not in stopwords]\n",
        "      l = [word for word in l if word not in punctuation]\n",
        "      l = \"\".join([word for word in l])\n",
        "      l = l.rstrip()\n",
        "      lines_embs.append(ftm.get_sentence_vector(l))\n",
        "    return lines_embs\n",
        "\n",
        "  def get_spacy_embeddings(self, f, nlp, lang):\n",
        "    file = open(f) \n",
        "    lines = file.readlines()\n",
        "    documents = nlp.pipe(lines, batch_size=32, n_threads=7)\n",
        "    lines_embs = []\n",
        "    for doc in documents:\n",
        "      l = []\n",
        "      for token in doc:\n",
        "        # if token.text in stopwords or token.text in punctuation:\n",
        "        #   continue\n",
        "        # if not token.has_vector:\n",
        "        #   l.append(unknown)\n",
        "        # else:\n",
        "        l.append(token.vector)\n",
        "      lines_embs.append(np.mean(np.array(l), axis=0))\n",
        "    return lines_embs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XX-5RfXknUZc",
        "colab_type": "code",
        "outputId": "1c8040c3-58f0-425e-a6a5-04df66c94467",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "# Creation of common objects that can be used for any types of model\n",
        "embedding = Embedding()\n",
        "embedding.load_spacy()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Collecting en_core_web_md==2.1.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.1.0/en_core_web_md-2.1.0.tar.gz (95.4MB)\n",
            "\u001b[K     |████████████████████████████████| 95.4MB 164.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: en-core-web-md\n",
            "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-md: filename=en_core_web_md-2.1.0-cp36-none-any.whl size=97126236 sha256=978e877f54555e18a5c6b62feee31765c3ca8c524566ab0b5af841d66bfc347b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7mb2yy8r/wheels/c1/2c/5f/fd7f3ec336bf97b0809c86264d2831c5dfb00fc2e239d1bb01\n",
            "Successfully built en-core-web-md\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_md -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en300\n",
            "You can now load the model via spacy.load('en300')\n",
            "Collecting de_core_news_md==2.1.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_md-2.1.0/de_core_news_md-2.1.0.tar.gz (220.8MB)\n",
            "\u001b[K     |████████████████████████████████| 220.8MB 1.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: de-core-news-md\n",
            "  Building wheel for de-core-news-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for de-core-news-md: filename=de_core_news_md-2.1.0-cp36-none-any.whl size=224546880 sha256=07bea2223085ca6bc9937b78ba961f740c79f6a2a7376954218f9f0b81d4b421\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qcizzkal/wheels/44/34/f1/31d4b0fa32008c09695ccb180865f196ecd9d512c146f99749\n",
            "Successfully built de-core-news-md\n",
            "Installing collected packages: de-core-news-md\n",
            "Successfully installed de-core-news-md-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_md')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/de_core_news_md -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/de300\n",
            "You can now load the model via spacy.load('de300')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAeI7hHwVLoS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preprocessing = Preprocessing()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4wo1kMlkEhp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fast Text Embedding\n",
        "def get_fast_text_embeddings():\n",
        "  embedding.download_fast_text()\n",
        "  embedding.load_fast_text()\n",
        "\n",
        "  concatenated_english_embs = preprocessing.get_fast_text_embeddings('./train.ende.src', \n",
        "                                                        embedding.nlp_en, \n",
        "                                                        embedding.stop_words_en, \n",
        "                                                        embedding.ft)\n",
        "  concatenated_german_embs = preprocessing.get_fast_text_embeddings('./train.ende.mt', \n",
        "                                                       embedding.nlp_de, \n",
        "                                                       embedding.stop_words_de, \n",
        "                                                       embedding.ft_de)\n",
        "\n",
        "  english_embs = []\n",
        "  for e in concatenated_english_embs:\n",
        "    sentence_embs = []\n",
        "    for i in range(0, len(e), 100):\n",
        "      sentence_embs.append(e[i : (i + 100)])\n",
        "    english_embs.append(sentence_embs)\n",
        "\n",
        "  german_embs = []\n",
        "  for e in concatenated_german_embs:\n",
        "    sentence_embs = []\n",
        "    for i in range(0, len(e), 100):\n",
        "      sentence_embs.append(e[i : (i + 100)])\n",
        "    german_embs.append(sentence_embs)\n",
        "  \n",
        "  return english_embs, german_embs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiPgPRRzy_ax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Spacy Embedding\n",
        "def get_spacy_embeddings(en_filepath, de_filepath):\n",
        "  english_embs = preprocessing.get_spacy_embeddings(en_filepath, embedding.nlp_en, 'en')\n",
        "  german_embs = preprocessing.get_spacy_embeddings(de_filepath, embedding.nlp_de, 'de')\n",
        "  return english_embs, german_embs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfQEvFC9llPX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_all_features(line_en, line_de, cosine_sim):\n",
        "  features = []\n",
        "\n",
        "  english_pos_counts = FeatureExtraction.get_pos_tag_counts(line_en, 'en', embedding.nlp_en)\n",
        "  german_pos_counts = FeatureExtraction.get_pos_tag_counts(line_de, 'de', embedding.nlp_de)\n",
        "  for k in english_pos_counts.keys():\n",
        "    features.append(english_pos_counts[k])\n",
        "\n",
        "  for k in german_pos_counts.keys():\n",
        "    features.append(german_pos_counts[k])\n",
        "\n",
        "  english_named_entities = FeatureExtraction.get_named_entities(line_en, 'en', embedding.nlp_en)\n",
        "  german_named_entities = FeatureExtraction.get_named_entities(line_de, 'de', embedding.nlp_de)\n",
        "  for k in english_named_entities.keys():\n",
        "    features.append(english_named_entities[k])\n",
        "\n",
        "  for k in german_named_entities.keys():\n",
        "    features.append(german_named_entities[k])\n",
        "\n",
        "  # for s in similarity:\n",
        "  #   features.append(s)\n",
        "\n",
        "  features.append(FeatureExtraction.get_num_of_tokens(line_en, 'en', embedding.nlp_en))\n",
        "  features.append(FeatureExtraction.get_num_of_tokens(line_de, 'de', embedding.nlp_de))\n",
        "  features.append(FeatureExtraction.get_avg_source_token_length(line_en, embedding.nlp_en))\n",
        "  features.append(FeatureExtraction.get_num_of_punctuations(line_en, embedding.nlp_en))\n",
        "  features.append(FeatureExtraction.get_num_of_punctuations(line_de, embedding.nlp_de))\n",
        "  # features.append(FeatureExtraction.get_num_of_numberic_tokens(line_en, embedding.nlp_en))\n",
        "  # features.append(FeatureExtraction.get_num_of_numberic_tokens(line_de, embedding.nlp_de))\n",
        "  features.append(FeatureExtraction.get_num_of_alpha_tokens(line_en, embedding.nlp_en))\n",
        "  features.append(FeatureExtraction.get_num_of_alpha_tokens(line_de, embedding.nlp_de))\n",
        "  features.append(FeatureExtraction.get_ratio_of_target_source_lengths(line_en, line_de, embedding.nlp_en, embedding.nlp_de))\n",
        "  features.append(FeatureExtraction.get_percentage_of_numbers_in_target(line_de, embedding.nlp_de))\n",
        "  # features.append(FeatureExtraction.get_dependency_tags(line_en))\n",
        "  features.append(cosine_sim)\n",
        "  \n",
        "  return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUqBxc9sD1b9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_combined_embeddings(english_emb, german_emb):\n",
        "  features = []\n",
        "  for value in english_emb:\n",
        "    features.append(value)\n",
        "  for value in german_emb:\n",
        "    features.append(value)\n",
        "\n",
        "  return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xa_rdSXdQ_kh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get embeddings - DON'T RUN FOR NOW\n",
        "english_embs, german_embs = get_spacy_embeddings(\"./train.ende.src\", \"./train.ende.mt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqtThqijoHmL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X_train_avgs = []\n",
        "# for i in range(len(english_embs)):\n",
        "#   X_train_avgs.append([np.array(english_embs[i]), np.array(german_embs[i])])\n",
        "\n",
        "with open('english_embs.txt', 'wb') as pickle_file:\n",
        "  pickle.dump(english_embs, pickle_file)\n",
        "with open('german_embs.txt', 'wb') as pickle_file:\n",
        "  pickle.dump(german_embs, pickle_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scLbTecBaB0p",
        "colab_type": "code",
        "outputId": "ca9085b4-3091-4f7a-a00e-fe7cd529edb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "# DON'T RUN FOR NOW\n",
        "\n",
        "padded_english_embs = np.empty(len(english_embs))\n",
        "for i in range(len(english_embs)):\n",
        "  padded_english_embs[i] = np.array(english_embs[i]).flatten()\n",
        "print(padded_english_embs)\n",
        "padded_english_embs = preprocessing.pad_sentences(padded_english_embs)\n",
        "\n",
        "padded_german_embs = np.empty(len(german_embs))\n",
        "for i in range(len(german_embs)):\n",
        "  padded_german_embs[i] = np.array(german_embs[i]).flatten()\n",
        "padded_german_embs = preprocessing.pad_sentences(padded_german_embs)\n",
        "\n",
        "similarities = FeatureExtraction.get_similarities(english_embs, german_embs)\n",
        "similarities = preprocessing.pad_sentences(similarities)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-9b385b0be0a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpadded_english_embs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menglish_embs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menglish_embs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mpadded_english_embs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menglish_embs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_english_embs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'english_embs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKAQ1U49EHP8",
        "colab_type": "code",
        "outputId": "41c5bd4c-6b17-4c8e-f883-0cb43aca4430",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# APPENDING WORD EMBEDDINGS OF ENGLISH AND GERMAN\n",
        "raw_embeddings = []\n",
        "for i in tqdm.tqdm(range(len(english_embs))):\n",
        "  raw_embeddings.append(get_combined_embeddings(english_embs[i], german_embs[i]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 7000/7000 [00:00<00:00, 16312.58it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvqmYRPfHaUp",
        "colab_type": "code",
        "outputId": "b59f6aef-afd4-4660-c12d-f351fc18bdd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "file = open('./train.ende.src') \n",
        "lines_en = file.readlines()\n",
        "file = open('./train.ende.mt')\n",
        "lines_de = file.readlines()\n",
        "\n",
        "train_cosine_sim_with_golden_translations = FeatureExtraction.get_cosine_sim_with_golden_translations('./train_google_translate_api.txt')\n",
        "\n",
        "samples = []\n",
        "for i in tqdm.tqdm(range(len(lines_en))):\n",
        "  samples.append(get_all_features(lines_en[i], lines_de[i], train_cosine_sim_with_golden_translations[i]))\n",
        "  # samples.append(get_all_features(lines_en[i], lines_de[i], similarities[i]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 7000/7000 [14:44<00:00,  7.23it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNlIQpkVTKW6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('train_features.txt', 'wb') as pickle_file:\n",
        "  pickle.dump(samples, pickle_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZlgO1yIM6_n",
        "colab_type": "text"
      },
      "source": [
        "# **Feature Selection**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KN_oEWSYNAgW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "# !pip install sklearn_crfsuite\n",
        "# import sklearn_crfsuite\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "from itertools import compress\n",
        "from scipy.stats import pearsonr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aA_WfUJ1bGiq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LOADING TRAIN SCORES\n",
        "train_scores_file = open(\"./train.ende.scores\",'r')\n",
        "train_scores = train_scores_file.readlines()\n",
        "\n",
        "file = open('./train_features.txt', 'rb')\n",
        "samples = pickle.load(file)\n",
        "file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHj-n8_KPGhA",
        "colab_type": "code",
        "outputId": "2012d6d9-9aa9-4127-9f27-fea6e37e2e5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "sel = VarianceThreshold(threshold=(.75 * (1 - .75)))\n",
        "X_train = sel.fit_transform(samples)\n",
        "print(len(X_train[0]))\n",
        "print(sel.get_support())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "34\n",
            "[ True  True  True False False  True  True False  True  True False  True\n",
            "  True  True False False  True  True False  True  True  True False  True\n",
            " False  True False  True  True  True  True  True  True False False  True\n",
            "  True False  True  True False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False False  True  True  True  True\n",
            "  True  True  True False False False]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1-ejCKXww5_",
        "colab_type": "code",
        "outputId": "ed17adf2-65fd-4a04-b7a8-c45e057b7432",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "model = BayesianRidge(n_iter=200, tol=0.001, alpha_1=1e-06, \n",
        "                      alpha_2=1e-06, lambda_1=1e-06, lambda_2=1e-06)\n",
        "# Initializing RFE model\n",
        "rfe = RFE(model, 27)\n",
        "\n",
        "clf = BayesianRidge(n_iter=200, tol=0.001, alpha_1=1e-06, \n",
        "                      alpha_2=1e-06, lambda_1=1e-06, lambda_2=1e-06)\n",
        "\n",
        "# Transforming data using RFE\n",
        "rfe.fit_transform(samples, train_scores)\n",
        "# Fitting the data to model\n",
        "samples_best_features = []\n",
        "for sample in samples:\n",
        "  samples_best_features.append(list(compress(sample, rfe.support_)))\n",
        "\n",
        "clf.fit(samples_best_features, train_scores)\n",
        "print(rfe.support_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:755: FutureWarning: Beginning in version 0.22, arrays of bytes/strings will be converted to decimal numbers if dtype='numeric'. It is recommended that you convert the array to a float dtype before using it in scikit-learn, for example by using your_array = your_array.astype(np.float64).\n",
            "  estimator=estimator)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[False False False False False False  True  True False  True False False\n",
            " False False False False  True  True False False False False  True  True\n",
            " False  True False  True  True  True False False False False False False\n",
            "  True False  True  True False False False False False False False  True\n",
            "  True  True False  True False  True False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False  True False False  True  True  True\n",
            "  True  True  True False False  True]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cnx80dZoTmZH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_src_file = open('./dev.ende.src') \n",
        "val_lines_en = val_src_file.readlines()\n",
        "val_mt_file = open('./dev.ende.mt')\n",
        "val_lines_de = val_mt_file.readlines()\n",
        "\n",
        "# val_samples = []\n",
        "# for i in tqdm.tqdm(range(len(val_lines_en))):\n",
        "#     val_samples.append(get_all_features(val_lines_en[i], val_lines_de[i]))\n",
        "file = open('./val_features.txt', 'rb')\n",
        "val_samples = pickle.load(file)\n",
        "file.close()\n",
        "\n",
        "\n",
        "val_samples_best_features = []\n",
        "for val_sample in val_samples:\n",
        "  val_samples_best_features.append(list(compress(val_sample, rfe.support_)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcioS6tqTrLA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('val_features.txt', 'wb') as pickle_file:\n",
        "  pickle.dump(val_samples, pickle_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uK21vKw-Trq5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_scores_file = open(\"./dev.ende.scores\",'r')\n",
        "val_scores = val_scores_file.readlines()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLumkInZTxqX",
        "colab_type": "code",
        "outputId": "09f3a8c5-248e-460c-b16c-563fd0a533ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predictions_svr = clf.predict(np.array(val_samples_best_features).astype(float))\n",
        "\n",
        "peason_cor, p_value = pearsonr(np.array(predictions_svr.flatten()), np.array(val_scores).astype(float))\n",
        "print(\"Pearson Correlation:\", peason_cor)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pearson Correlation: 0.1399618372252919\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOagQEG6crNS",
        "colab_type": "code",
        "outputId": "5e9be291-c7f1-4581-aee4-f35bf6a8a023",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        }
      },
      "source": [
        "# GENERATE TEST SET FEATURES\n",
        "file = open('./test.ende.src') \n",
        "lines_en = file.readlines()\n",
        "file = open('./test.ende.mt')\n",
        "lines_de = file.readlines()\n",
        "\n",
        "test_samples = []\n",
        "for i in tqdm.tqdm(range(len(lines_en))):\n",
        "    test_samples.append(get_all_features(lines_en[i], lines_de[i]))\n",
        "\n",
        "test_samples_best_features = []\n",
        "for test_sample in test_samples:\n",
        "  test_samples_best_features.append(list(compress(test_sample, rfe.support_)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1000 [00:00<?, ?it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-100-f876aa00cfd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtest_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines_en\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtest_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_all_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines_en\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines_de\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtest_samples_best_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: get_all_features() missing 1 required positional argument: 'cosine_sim'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKCRuHFyc2o0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('test_samples.txt', 'wb') as pickle_file:\n",
        "  pickle.dump(test_samples, pickle_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOoizRardERu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_best_features_predictions = clf.predict(test_samples_best_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4DoOXOMdIa3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def write_test_predications(test_predictions):\n",
        "  f = open(\"predictions.txt\", \"w\")\n",
        "  for num in test_predictions:\n",
        "    f.write(f\"{num}\\n\")\n",
        "  f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RrLtruIdOwc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "write_test_predications(test_best_features_predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h76geFb-dsdj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "zipObj = ZipFile('predictions.txt.zip', 'w')\n",
        " \n",
        "# Add multiple files to the zip\n",
        "zipObj.write('predictions.txt')\n",
        " \n",
        "# close the Zip File\n",
        "zipObj.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwit37bCokcE",
        "colab_type": "text"
      },
      "source": [
        "# **Regressors**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJYMH5hiQVtD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "# !pip install sklearn_crfsuite\n",
        "# import sklearn_crfsuite\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "from itertools import compress\n",
        "from scipy.stats import pearsonr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MFnMO-BbFSk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LOADING TRAIN SCORES\n",
        "train_scores_file = open(\"./train.ende.scores\",'r')\n",
        "train_scores = train_scores_file.readlines()\n",
        "\n",
        "file = open('./train_features.txt', 'rb')\n",
        "samples = pickle.load(file)\n",
        "file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iEJ-4EzQgA9",
        "colab_type": "code",
        "outputId": "83165956-38f3-428e-e056-72c4f38ce4e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "clf = SVR(C=10, epsilon=0.01, gamma=0.0001)\n",
        "clf.fit(np.array(samples), np.array(train_scores))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ True  True  True  True False  True  True  True  True  True  True  True\n",
            "  True  True False  True  True  True False False False False False False\n",
            " False False False False  True False False False False False False False\n",
            " False False  True False  True False False  True  True  True  True False\n",
            "  True  True False False  True  True False  True False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False  True  True False False False\n",
            " False False False False False]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3Hd4ETYzmm3",
        "colab_type": "code",
        "outputId": "cca2ff51-6408-4abd-8e49-23861218231a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "br = BayesianRidge()\n",
        "br.fit(np.array(samples), np.array(train_scores))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, alpha_init=None,\n",
              "              compute_score=False, copy_X=True, fit_intercept=True,\n",
              "              lambda_1=1e-06, lambda_2=1e-06, lambda_init=None, n_iter=300,\n",
              "              normalize=False, tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VV5VHBfMnQzJ",
        "colab_type": "code",
        "outputId": "5181075b-982f-4410-d9c8-5d136bf874ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "rfr = RandomForestRegressor()\n",
        "rfr.fit(np.array(samples), np.array(train_scores))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
              "                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
              "                      max_samples=None, min_impurity_decrease=0.0,\n",
              "                      min_impurity_split=None, min_samples_leaf=1,\n",
              "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
              "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
              "                      random_state=None, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFZZSWgVo_9k",
        "colab_type": "code",
        "outputId": "8188654e-d39b-4d12-d78e-401e77df8585",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "for count, degree in tqdm.tqdm(enumerate([1, 2])):\n",
        "    model = make_pipeline(PolynomialFeatures(degree), SVR(C=10, epsilon=0.01, gamma=0.0001))\n",
        "    model.fit(np.array(samples), np.array(train_scores))\n",
        "    predictions_poly = model.predict(val_samples)\n",
        "    peason_cor, p_value = pearsonr(np.array(predictions_poly.flatten()), np.array(val_scores).astype(float))\n",
        "    print(\"Pearson Correlation:\", peason_cor)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-2b0de585e219>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPolynomialFeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSVR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mpredictions_poly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mpeason_cor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpearsonr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions_poly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pearson Correlation:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeason_cor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'val_samples' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZy-FzwkII0y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DON'T RUN - Doesn't work\n",
        "!pip install sklearn_crfsuite\n",
        "import sklearn_crfsuite\n",
        "\n",
        "crf = sklearn_crfsuite.CRF(algorithm='lbfgs', c1=0.1,c2=0.1,max_iterations=100,all_possible_transitions=True)\n",
        "crf.fit(np.array(samples), np.array(de_train_scores))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2xjgL36XixE",
        "colab_type": "code",
        "outputId": "9d37e8da-7040-4518-88f3-9f640518b477",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "br = BayesianRidge()\n",
        "br.fit(np.array(raw_embeddings), np.array(train_scores))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, alpha_init=None,\n",
              "              compute_score=False, copy_X=True, fit_intercept=True,\n",
              "              lambda_1=1e-06, lambda_2=1e-06, lambda_init=None, n_iter=300,\n",
              "              normalize=False, tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQnVjT1jQPy-",
        "colab_type": "text"
      },
      "source": [
        "#**Neural Networks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmOcwBkPla8-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import optimizers\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from numpy.random import seed\n",
        "seed(1)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DJegx6IlghL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(16, input_dim=25, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(4, activation='relu'))\n",
        "# model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "\n",
        "# optimizer = optimizers.RMSprop(0.001)\n",
        "\n",
        "model.compile(loss='mse',\n",
        "              optimizer='Adam',\n",
        "              metrics=['mae', 'mse'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDpxz8fTlaAq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f_train_scores = open(\"./train.ende.scores\",'r')\n",
        "de_train_scores = f_train_scores.readlines()\n",
        "y_train = []\n",
        "for s in de_train_scores:\n",
        "  y_train.append(float(s))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KP7pb-TZwyrq",
        "colab_type": "code",
        "outputId": "030800fa-2b06-4528-fac5-8e0b85caac9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(max(samples_best_features, key=len)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTzoVOG1nVEO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "es=EarlyStopping(monitor='val_loss', mode='min', verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPqtysw5uMzO",
        "colab_type": "code",
        "outputId": "d2bdcb2f-126b-4011-bbb4-1d581e4bcd13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "model.fit(np.array(samples_best_features), \n",
        "          np.array(de_train_scores),\n",
        "          epochs=4,\n",
        "          validation_data=(np.array(val_samples_best_features[:500]), np.array(val_scores[:500]).astype(float)),\n",
        "          # callbacks=[es], \n",
        "          verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 7000 samples, validate on 500 samples\n",
            "Epoch 1/4\n",
            "7000/7000 [==============================] - 2s 356us/step - loss: 0.8118 - mean_absolute_error: 0.5440 - mean_squared_error: 0.8118 - val_loss: 0.8931 - val_mean_absolute_error: 0.5739 - val_mean_squared_error: 0.8931\n",
            "Epoch 2/4\n",
            "7000/7000 [==============================] - 0s 60us/step - loss: 0.6830 - mean_absolute_error: 0.4896 - mean_squared_error: 0.6830 - val_loss: 0.8914 - val_mean_absolute_error: 0.5818 - val_mean_squared_error: 0.8914\n",
            "Epoch 3/4\n",
            "7000/7000 [==============================] - 0s 60us/step - loss: 0.6817 - mean_absolute_error: 0.4916 - mean_squared_error: 0.6817 - val_loss: 0.8916 - val_mean_absolute_error: 0.5807 - val_mean_squared_error: 0.8916\n",
            "Epoch 4/4\n",
            "7000/7000 [==============================] - 0s 58us/step - loss: 0.6816 - mean_absolute_error: 0.4910 - mean_squared_error: 0.6816 - val_loss: 0.8917 - val_mean_absolute_error: 0.5799 - val_mean_squared_error: 0.8917\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f48fe73e978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 316
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0JHvPfynwqU",
        "colab_type": "code",
        "outputId": "2306211c-771c-4f73-cbdf-549dc5f93900",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predictions_model = model.predict(np.array(val_samples_best_features[500:]))\n",
        "\n",
        "peason_cor, p_value = pearsonr(np.array(predictions_model.flatten()), np.array(val_scores[500:]).astype(float))\n",
        "print(\"Pearson Correlation:\", peason_cor)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pearson Correlation: 0.09324744930482495\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gv5WYutApZ13",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_best_features_predictions = model.predict(np.array(test_samples_best_features))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6epYnV3p66S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def write_test_predications(test_predictions):\n",
        "  f = open(\"predictions.txt\", \"w\")\n",
        "  for num in test_predictions:\n",
        "    f.write(f\"{num[0]}\\n\")\n",
        "  f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCn-3roYp7og",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "write_test_predications(test_best_features_predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5i393GVqGIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "zipObj = ZipFile('predictions-6.txt.zip', 'w')\n",
        " \n",
        "# Add multiple files to the zip\n",
        "zipObj.write('predictions.txt')\n",
        " \n",
        "# close the Zip File\n",
        "zipObj.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwOnVIFKqICo",
        "colab_type": "text"
      },
      "source": [
        "# **Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CL04IvbjTj0A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.stats import pearsonr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdnUX6Axm4jL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_cosine_sim_with_golden_translations = FeatureExtraction.get_cosine_sim_with_golden_translations('./val_google_translate_api.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ue5jhfU6SB0s",
        "colab_type": "code",
        "outputId": "0046a462-40e0-4f2a-aeba-06c0de333b0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "val_src_file = open('./dev.ende.src') \n",
        "val_lines_en = val_src_file.readlines()\n",
        "val_mt_file = open('./dev.ende.mt')\n",
        "val_lines_de = val_mt_file.readlines()\n",
        "\n",
        "val_samples = []\n",
        "for i in tqdm.tqdm(range(len(val_lines_en))):\n",
        "    val_samples.append(get_all_features(val_lines_en[i], val_lines_de[i], val_cosine_sim_with_golden_translations[i]))\n",
        "    # samples2.append(get_all_features(val_lines_en[i], val_lines_de[i], similarities2[i]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [02:04<00:00,  8.14it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k92Y3P_-ZJ8R",
        "colab_type": "code",
        "outputId": "a5aaaa44-4c99-492c-875c-f758b85142d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "val_src_file = open('./dev.ende.src') \n",
        "val_lines_en = val_src_file.readlines()\n",
        "val_mt_file = open('./dev.ende.mt')\n",
        "val_lines_de = val_mt_file.readlines()\n",
        "\n",
        "val_english_embs, val_german_embs = get_spacy_embeddings('./dev.ende.src', './dev.ende.mt')\n",
        "\n",
        "val_samples = []\n",
        "for i in tqdm.tqdm(range(len(val_lines_en))):\n",
        "    val_samples.append(get_combined_embeddings(val_english_embs[i], val_german_embs[i]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:00<00:00, 17253.41it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYGuAYoBE8ST",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('val_features.txt', 'wb') as pickle_file:\n",
        "  pickle.dump(val_samples, pickle_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLwIs9NTSCAz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_scores_file = open(\"./dev.ende.scores\",'r')\n",
        "val_scores = val_scores_file.readlines()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwiyGxNISfoO",
        "colab_type": "code",
        "outputId": "40d409fc-de66-41fc-a622-db3d250fc7d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predictions_svr = clf.predict(np.array(val_samples))\n",
        "\n",
        "peason_cor, p_value = pearsonr(np.array(predictions_svr.flatten()), np.array(val_scores).astype(float))\n",
        "print(\"Pearson Correlation:\", peason_cor)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pearson Correlation: 0.1086595794000567\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rx4sE7Vzw8L",
        "colab_type": "code",
        "outputId": "2f9f0c23-1d13-40bf-b6bc-b825e6d6a607",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predictions_br = br.predict(np.array(val_samples).astype(float))\n",
        "\n",
        "peason_cor, p_value = pearsonr(np.array(predictions_br.flatten()), np.array(val_scores).astype(float))\n",
        "print(\"Pearson Correlation:\", peason_cor)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pearson Correlation: 0.12418641442186172\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pIAx9usnZ99",
        "colab_type": "code",
        "outputId": "6647ee06-09a2-4726-f2d4-05fb4e35713c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predictions_rfr = rfr.predict(np.array(val_samples))\n",
        "\n",
        "peason_cor, p_value = pearsonr(np.array(predictions_rfr.flatten()), np.array(val_scores).astype(float))\n",
        "print(\"Pearson Correlation:\", peason_cor)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pearson Correlation: 0.06050073430696995\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJGUoTVhTqaN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59zCXMV_Tt5i",
        "colab_type": "text"
      },
      "source": [
        "## **Test Predictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQK0VSpS04R1",
        "colab_type": "code",
        "outputId": "d403090b-ad3f-47e3-8bd8-298dd1d40365",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# GENERATE TEST SET FEATURES\n",
        "file = open('./test.ende.src') \n",
        "lines_en = file.readlines()\n",
        "file = open('./test.ende.mt')\n",
        "lines_de = file.readlines()\n",
        "\n",
        "test_samples = []\n",
        "for i in tqdm.tqdm(range(len(lines_en))):\n",
        "    test_samples.append(get_all_features(lines_en[i], lines_de[i]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:00<00:00, 284128.44it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XrLXwPxbHhK",
        "colab_type": "code",
        "outputId": "d08b5afa-89bf-429b-e3a3-ceb77eefec53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# GENERATE TEST SET FEATURES\n",
        "test_english_embs, test_german_embs = get_spacy_embeddings('./test.ende.src', './test.ende.mt')\n",
        "\n",
        "test_samples = []\n",
        "for i in tqdm.tqdm(range(len(test_english_embs))):\n",
        "    test_samples.append(get_combined_embeddings(test_english_embs[i], test_german_embs[i]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:00<00:00, 17033.88it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEm0SlluF6ls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('test_samples.txt', 'wb') as pickle_file:\n",
        "  pickle.dump(test_samples, pickle_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DZKmAOGAKmw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_predictions = br.predict(test_samples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yj8-Be8oC9PA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def write_test_predications(test_predictions):\n",
        "  f = open(\"predictions.txt\", \"w\")\n",
        "  for num in test_predictions:\n",
        "    f.write(f\"{num}\\n\")\n",
        "  f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0b8KHSiz--p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "write_test_predications(test_predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuzFmlambheX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "zipObj = ZipFile('predictions.txt.zip', 'w')\n",
        " \n",
        "# Add multiple files to the zip\n",
        "zipObj.write('predictions.txt')\n",
        " \n",
        "# close the Zip File\n",
        "zipObj.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bn_Ou1WoboMZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}