{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Features_models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cM3LSopQW3Ab",
        "colab_type": "text"
      },
      "source": [
        "#**1. IMPORTING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhhK1M7LW7Sj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from os.path import exists\n",
        "if not exists('ende_data.zip'):\n",
        "    !wget -O ende_data.zip https://competitions.codalab.org/my/datasets/download/c748d2c0-d6be-4e36-9f12-ca0e88819c4d\n",
        "    !unzip ende_data.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAz_War7Qy10",
        "colab_type": "code",
        "outputId": "011c2446-f447-4d5f-bfcd-45e57b441a16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if not exists('requirements.txt'):\n",
        "  !wget https://raw.githubusercontent.com/Unbabel/KiwiCutter/master/requirements.txt\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-28 18:12:11--  https://raw.githubusercontent.com/Unbabel/KiwiCutter/master/requirements.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1419 (1.4K) [text/plain]\n",
            "Saving to: ‘requirements.txt’\n",
            "\n",
            "\rrequirements.txt      0%[                    ]       0  --.-KB/s               \rrequirements.txt    100%[===================>]   1.39K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-02-28 18:12:11 (220 MB/s) - ‘requirements.txt’ saved [1419/1419]\n",
            "\n",
            "Collecting alembic==1.0.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/8b/0c98c378d93165d9809193f274c3c6e2151120d955b752419c7d43e4d857/alembic-1.0.11.tar.gz (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 2.7MB/s \n",
            "\u001b[?25hCollecting appnope==0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/87/a9/7985e6a53402f294c8f0e8eff3151a83f1fb901fa92909bb3ff29b4d22af/appnope-0.1.0-py2.py3-none-any.whl\n",
            "Collecting attrs==19.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/23/96/d828354fa2dbdf216eaa7b7de0db692f12c234f7ef888cc14980ef40d1d2/attrs-19.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: backcall==0.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (0.1.0)\n",
            "Requirement already satisfied: bleach==3.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (3.1.0)\n",
            "Collecting certifi==2019.6.16\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/1b/b853c7a9d4f6a6d00749e94eb6f3a041e342a885b87340b79c1ef73e3a78/certifi-2019.6.16-py2.py3-none-any.whl (157kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 42.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (3.0.4)\n",
            "Requirement already satisfied: Click==7.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (7.0)\n",
            "Collecting cloudpickle==1.2.1\n",
            "  Downloading https://files.pythonhosted.org/packages/09/f4/4a080c349c1680a2086196fcf0286a65931708156f39568ed7051e42ff6a/cloudpickle-1.2.1-py2.py3-none-any.whl\n",
            "Collecting colored==1.3.93\n",
            "  Downloading https://files.pythonhosted.org/packages/58/07/636616667b47d3115b0288311511c5fb446d0e499036b7db858704c89066/colored-1.3.93.tar.gz\n",
            "Collecting ConfigArgParse==0.14.0\n",
            "  Downloading https://files.pythonhosted.org/packages/55/ea/f0ade52790bcd687127a302b26c1663bf2e0f23210d5281dbfcd1dfcda28/ConfigArgParse-0.14.0.tar.gz\n",
            "Collecting configparser==3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/ab/1a/ec151e5e703ac80041eaccef923611bbcec2b667c20383655a06962732e9/configparser-3.8.1-py2.py3-none-any.whl\n",
            "Collecting databricks-cli==0.8.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5f/38/f83bc71c5e7351a03e8d44aaf04647d076bbf8f097e3f93b921704b7a74c/databricks_cli-0.8.7-py3-none-any.whl (82kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 10.0MB/s \n",
            "\u001b[?25hCollecting ddt==1.2.1\n",
            "  Downloading https://files.pythonhosted.org/packages/cf/f5/f83dea32dc3fb3be1e5afab8438dce73ed587740a2a061ae2ea56e04a36d/ddt-1.2.1-py2.py3-none-any.whl\n",
            "Collecting decorator==4.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/5f/88/0075e461560a1e750a0dcbf77f1d9de775028c37a19a346a6c565a257399/decorator-4.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: defusedxml==0.6.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 16)) (0.6.0)\n",
            "Collecting docker==4.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/47/5560c9cf0c92b50da24216f0e7733250fbed5a497f69e3c70e1be62143fe/docker-4.0.2-py2.py3-none-any.whl (138kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 43.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: entrypoints==0.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 18)) (0.3)\n",
            "Requirement already satisfied: Flask==1.1.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 19)) (1.1.1)\n",
            "Collecting gitdb2==2.0.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/30/a407568aa8d8f25db817cf50121a958722f3fc5f87e3a6fba1f40c0633e3/gitdb2-2.0.5-py2.py3-none-any.whl (62kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.1MB/s \n",
            "\u001b[?25hCollecting GitPython==3.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/c9/418ec7a9575f58c3670bab5caebc2cb28d6ac4cd6f52074bf75ab3ef2a28/GitPython-3.0.1-py3-none-any.whl (453kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 30.4MB/s \n",
            "\u001b[?25hCollecting gorilla==0.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e3/56/5a683944cbfc77e429c6f03c636ca50504a785f60ffae91ddd7f5f7bb520/gorilla-0.3.0-py2.py3-none-any.whl\n",
            "Collecting gunicorn==19.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/da/b8dd8deb741bff556db53902d4706774c8e1e67265f69528c14c003644e6/gunicorn-19.9.0-py2.py3-none-any.whl (112kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 46.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna==2.8 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 24)) (2.8)\n",
            "Collecting ipykernel==5.1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/16/43f51f65a8a08addf04f909a0938b06ba1ee1708b398a9282474531bd893/ipykernel-5.1.2-py3-none-any.whl (116kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 46.0MB/s \n",
            "\u001b[?25hCollecting ipython==7.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/c4/a79582814bdfe92bfca4d286a729304ffdf13f5135132cfcaea13cf1b2b3/ipython-7.7.0-py3-none-any.whl (774kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 40.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipython-genutils==0.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 27)) (0.2.0)\n",
            "Requirement already satisfied: ipywidgets==7.5.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 28)) (7.5.1)\n",
            "Requirement already satisfied: itsdangerous==1.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 29)) (1.1.0)\n",
            "Collecting jedi==0.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/55/54/da994f359e4e7da4776a200e76dbc85ba5fc319eefc22e33d55296d95a1d/jedi-0.15.1-py2.py3-none-any.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 40.2MB/s \n",
            "\u001b[?25hCollecting Jinja2==2.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/e7/fd8b501e7a6dfe492a433deb7b9d833d39ca74916fa8bc63dd1a4947a671/Jinja2-2.10.1-py2.py3-none-any.whl (124kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 40.9MB/s \n",
            "\u001b[?25hCollecting jsonschema==3.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/48/f5f11003ceddcd4ad292d4d9b5677588e9169eef41f88e38b2888e7ec6c4/jsonschema-3.0.2-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.2MB/s \n",
            "\u001b[?25hCollecting jupyter-client==5.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/4c/bf613864ae0644e2ac7d4a40bd209c40c8c71e3dc88d5f1d0aa92a68e716/jupyter_client-5.3.1-py2.py3-none-any.whl (91kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 6.2MB/s \n",
            "\u001b[?25hCollecting jupyter-core==4.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/25/6ffb0f6e57fa6ef5d2f814377133b361b42a6dd39105f4885a4f1666c2c3/jupyter_core-4.5.0-py2.py3-none-any.whl (78kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.0MB/s \n",
            "\u001b[?25hCollecting Mako==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/3c/8dcd6883d009f7cae0f3157fb53e9afb05a0d3d33b3db1268ec2e6f4a56b/Mako-1.1.0.tar.gz (463kB)\n",
            "\u001b[K     |████████████████████████████████| 471kB 46.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe==1.1.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 36)) (1.1.1)\n",
            "Requirement already satisfied: mistune==0.8.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 37)) (0.8.4)\n",
            "Collecting more-itertools==5.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/a6/42f17d065bda1fac255db13afc94c93dbfb64393eae37c749b4cb0752fc7/more_itertools-5.0.0-py3-none-any.whl (52kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.1MB/s \n",
            "\u001b[?25hCollecting nbconvert==5.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/df/4505c0a7fea624cac461d0f41051f33456ae656753f65cee8c2f43121cb2/nbconvert-5.6.0-py2.py3-none-any.whl (453kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 41.7MB/s \n",
            "\u001b[?25hCollecting nbformat==4.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/27/9a654d2b6cc1eaa517d1c5a4405166c7f6d72f04f6e7eea41855fe808a46/nbformat-4.4.0-py2.py3-none-any.whl (155kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 40.9MB/s \n",
            "\u001b[?25hCollecting nltk==3.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 33.3MB/s \n",
            "\u001b[?25hCollecting notebook==6.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/a1/1e07cedcb554408fefe4a7d32b2a041c86517167aec6ca8251c808ef6c1e/notebook-6.0.1-py3-none-any.whl (9.0MB)\n",
            "\u001b[K     |████████████████████████████████| 9.0MB 31.5MB/s \n",
            "\u001b[?25hCollecting numpy==1.17.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/b9/bda9781f0a74b90ebd2e046fde1196182900bd4a8e1ea503d3ffebc50e7c/numpy-1.17.0-cp36-cp36m-manylinux1_x86_64.whl (20.4MB)\n",
            "\u001b[K     |████████████████████████████████| 20.4MB 163kB/s \n",
            "\u001b[?25hCollecting openkiwi==0.1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/74/767e788ed15f4c7cf093d8013d81bfb86c7c9a09816bd557a52c7d948d46/openkiwi-0.1.2-py3-none-any.whl (174kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 33.6MB/s \n",
            "\u001b[?25hCollecting pandas==0.25.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/9a/7eb9952f4b4d73fbd75ad1d5d6112f407e695957444cb695cbb3cdab918a/pandas-0.25.0-cp36-cp36m-manylinux1_x86_64.whl (10.5MB)\n",
            "\u001b[K     |████████████████████████████████| 10.5MB 36.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandocfilters==1.4.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 46)) (1.4.2)\n",
            "Collecting parso==0.5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/bd/bf4e5bd01d79906e5b945a7af033154da49fd2b0d5b5c705a21330323305/parso-0.5.1-py2.py3-none-any.whl (95kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 9.1MB/s \n",
            "\u001b[?25hCollecting pexpect==4.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/3e/377007e3f36ec42f1b84ec322ee12141a9e10d808312e5738f52f80a232c/pexpect-4.7.0-py2.py3-none-any.whl (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pickleshare==0.7.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 49)) (0.7.5)\n",
            "Requirement already satisfied: prometheus-client==0.7.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 50)) (0.7.1)\n",
            "Collecting prompt-toolkit==2.0.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/a7/9b1dd14ef45345f186ef69d175bdd2491c40ab1dfa4b2b3e4352df719ed7/prompt_toolkit-2.0.9-py3-none-any.whl (337kB)\n",
            "\u001b[K     |████████████████████████████████| 337kB 43.4MB/s \n",
            "\u001b[?25hCollecting protobuf==3.9.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/f4/a27952733796330cd17c17ea1f974459f5fefbbad119c0f296a6d807fec3/protobuf-3.9.1-cp36-cp36m-manylinux1_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 39.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: ptyprocess==0.6.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 53)) (0.6.0)\n",
            "Collecting Pygments==2.4.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/73/1dfa428150e3ccb0fa3e68db406e5be48698f2a979ccbcec795f28f44048/Pygments-2.4.2-py2.py3-none-any.whl (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 35.1MB/s \n",
            "\u001b[?25hCollecting pyrsistent==0.15.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/66/b2638d96a2d128b168d0dba60fdc77b7800a9b4a5340cefcc5fc4eae6295/pyrsistent-0.15.4.tar.gz (107kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 43.7MB/s \n",
            "\u001b[?25hCollecting python-dateutil==2.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/17/c62faccbfbd163c7f57f3844689e3a78bae1f403648a6afb1d0866d87fbb/python_dateutil-2.8.0-py2.py3-none-any.whl (226kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 42.2MB/s \n",
            "\u001b[?25hCollecting python-editor==1.0.4\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\n",
            "Collecting pytz==2019.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/76/46d697698a143e05f77bec5a526bf4e56a0be61d63425b68f4ba553b51f2/pytz-2019.2-py2.py3-none-any.whl (508kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 50.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML==3.13 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 59)) (3.13)\n",
            "Collecting pyzmq==18.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/89/6f0ea51ffa9c2c00c0ab0460f137b16a5ab5b47e3b060c5b1fc9ca425836/pyzmq-18.1.0-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 45.3MB/s \n",
            "\u001b[?25hCollecting querystring-parser==1.2.4\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/fa/f54f5662e0eababf0c49e92fd94bf178888562c0e7b677c8941bbbcd1bd6/querystring_parser-1.2.4.tar.gz\n",
            "Collecting requests==2.22.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.2MB/s \n",
            "\u001b[?25hCollecting scipy==1.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/50/a552a5aff252ae915f522e44642bb49a7b7b31677f9580cfd11bcc869976/scipy-1.3.1-cp36-cp36m-manylinux1_x86_64.whl (25.2MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2MB 1.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: Send2Trash==1.5.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 64)) (1.5.0)\n",
            "Collecting simplejson==3.16.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/24/c35fb1c1c315fc0fffe61ea00d3f88e85469004713dab488dee4f35b0aff/simplejson-3.16.0.tar.gz (81kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six==1.12.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 66)) (1.12.0)\n",
            "Collecting smmap2==2.0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/55/d2/866d45e3a121ee15a1dc013824d58072fd5c7799c9c34d01378eb262ca8f/smmap2-2.0.5-py2.py3-none-any.whl\n",
            "Collecting SQLAlchemy==1.3.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c2/75/6217c626fa22ad56ae5ccb1a36e7c4f17f5ca31543887e00179468d10464/SQLAlchemy-1.3.7.tar.gz (5.9MB)\n",
            "\u001b[K     |████████████████████████████████| 5.9MB 25.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlparse==0.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 69)) (0.3.0)\n",
            "Collecting tabulate==0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c2/fd/202954b3f0eb896c53b7b6f07390851b1fd2ca84aa95880d7ae4f434c4ac/tabulate-0.8.3.tar.gz (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.5MB/s \n",
            "\u001b[?25hCollecting terminado==0.8.2\n",
            "  Downloading https://files.pythonhosted.org/packages/a7/56/80ea7fa66565fa75ae21ce0c16bc90067530e5d15e48854afcc86585a391/terminado-0.8.2-py2.py3-none-any.whl\n",
            "Collecting testpath==0.4.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/be/a4/162f9ebb6489421fe46dcca2ae420369edfee4b563c668d93cb4605d12ba/testpath-0.4.2-py2.py3-none-any.whl (163kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 41.2MB/s \n",
            "\u001b[?25hCollecting torch==1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/57/d5cceb0799c06733eefce80c395459f28970ebb9e896846ce96ab579a3f1/torch-1.2.0-cp36-cp36m-manylinux1_x86_64.whl (748.8MB)\n",
            "\u001b[K     |████████████████████████████████| 748.9MB 17kB/s \n",
            "\u001b[?25hRequirement already satisfied: torchtext==0.3.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 74)) (0.3.1)\n",
            "Collecting tornado==6.0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/78/2d2823598496127b21423baffaa186b668f73cd91887fcef78b6eade136b/tornado-6.0.3.tar.gz (482kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 34.9MB/s \n",
            "\u001b[?25hCollecting tqdm==4.34.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/83/06029af22fe06b8a7be013aeae5e104b3ed26867e5d4ca91408b30aa602e/tqdm-4.34.0-py2.py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.0MB/s \n",
            "\u001b[?25hCollecting traitlets==4.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/d6/abcb22de61d78e2fc3959c964628a5771e47e7cc60d53e9342e21ed6cc9a/traitlets-4.3.2-py2.py3-none-any.whl (74kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 7.3MB/s \n",
            "\u001b[?25hCollecting urllib3==1.25.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/60/247f23a7121ae632d62811ba7f273d0e58972d75e58a94d329d51550a47d/urllib3-1.25.3-py2.py3-none-any.whl (150kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 41.0MB/s \n",
            "\u001b[?25hCollecting wcwidth==0.1.7\n",
            "  Downloading https://files.pythonhosted.org/packages/7e/9f/526a6947247599b084ee5232e4f9190a38f398d7300d866af3ab571a5bfe/wcwidth-0.1.7-py2.py3-none-any.whl\n",
            "Requirement already satisfied: webencodings==0.5.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 80)) (0.5.1)\n",
            "Collecting websocket-client==0.56.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/19/44753eab1fdb50770ac69605527e8859468f3c0fd7dc5a76dd9c4dbd7906/websocket_client-0.56.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 33.2MB/s \n",
            "\u001b[?25hCollecting Werkzeug==0.15.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/ab/d3bed6b92042622d24decc7aadc8877badf18aeca1571045840ad4956d3f/Werkzeug-0.15.5-py2.py3-none-any.whl (328kB)\n",
            "\u001b[K     |████████████████████████████████| 337kB 38.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: widgetsnbextension==3.5.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 83)) (3.5.1)\n",
            "Collecting yamlmagic==0.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/bd/db/d677d565b9048b9003ae6aac3ec34cce9dcc0e9c13bd68289c7c8dde3959/yamlmagic-0.2.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython==7.7.0->-r requirements.txt (line 26)) (45.1.0)\n",
            "Building wheels for collected packages: alembic, colored, ConfigArgParse, Mako, nltk, pyrsistent, querystring-parser, simplejson, SQLAlchemy, tabulate, tornado\n",
            "  Building wheel for alembic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for alembic: filename=alembic-1.0.11-py2.py3-none-any.whl size=162176 sha256=097405a847e4b214e62c43e30a6269149121651542287c4e31307de19d3b5b10\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/65/b2/9837b4422d13e739c3324c428f1b3aa9e3c3df666bb420e4b3\n",
            "  Building wheel for colored (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for colored: filename=colored-1.3.93-cp36-none-any.whl size=12575 sha256=0093544e2c99c5297f2595973f498baace6c6de710394cd033e7d61e01d0e2ba\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/c3/b7/ac21460710230feb409fee89bf594c4f2660ff7b67491d128f\n",
            "  Building wheel for ConfigArgParse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ConfigArgParse: filename=ConfigArgParse-0.14.0-cp36-none-any.whl size=17504 sha256=81f80ec683da412c39679654fc16a6e42bdfef02b6f4daba02107a3b8bc4e765\n",
            "  Stored in directory: /root/.cache/pip/wheels/aa/9c/ce/7e904dddb8c7595ffbe3409d24455bc5005852850e36011bda\n",
            "  Building wheel for Mako (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Mako: filename=Mako-1.1.0-cp36-none-any.whl size=75362 sha256=4092a441740b44ce4dc50597241f72b5288bc0ec4be0ed4887a31671d4eb84db\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/32/7b/a291926643fc1d1e02593e0d9e247c5a866a366b8343b7aa27\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-cp36-none-any.whl size=1449908 sha256=ba1121313e59cb0679a2c9216c2154805bc2b43ab6e3a250131440168166d60d\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
            "  Building wheel for pyrsistent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyrsistent: filename=pyrsistent-0.15.4-cp36-cp36m-linux_x86_64.whl size=97548 sha256=24a8f9c703eaaf2a551572c366002f67c54a9442519a1b1997f5d237fc21f6c0\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/46/00/6d471ef0b813e3621f0abe6cb723c20d529d39a061de3f7c51\n",
            "  Building wheel for querystring-parser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for querystring-parser: filename=querystring_parser-1.2.4-cp36-none-any.whl size=7079 sha256=b451ea40903a54c4f26ce7fb55a53e01f1232abf5c0c8a79b3ed42212d265f03\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/41/34/23ebf5d1089a9aed847951e0ee375426eb4ad0a7079d88d41e\n",
            "  Building wheel for simplejson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for simplejson: filename=simplejson-3.16.0-cp36-cp36m-linux_x86_64.whl size=114027 sha256=3d4d1abd6e0f8d7279e0d62187186ae5f8f74a3127462610040348e3995eca01\n",
            "  Stored in directory: /root/.cache/pip/wheels/5d/1a/1e/0350bb3df3e74215cd91325344cc86c2c691f5306eb4d22c77\n",
            "  Building wheel for SQLAlchemy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for SQLAlchemy: filename=SQLAlchemy-1.3.7-cp36-cp36m-linux_x86_64.whl size=1191605 sha256=80b202e5684407ec97ee64bbe74813d8cb94255acd21299ffaa5eea470daa549\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/e0/68/3d11cc7209b5bd2c7d55cbb56c6bda843cc82f77c8387468ea\n",
            "  Building wheel for tabulate (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tabulate: filename=tabulate-0.8.3-cp36-none-any.whl size=23379 sha256=ac94edb5673e85bef1b42a23378fab71d1aec91e9027f2cc093abede77280c08\n",
            "  Stored in directory: /root/.cache/pip/wheels/2b/67/89/414471314a2d15de625d184d8be6d38a03ae1e983dbda91e84\n",
            "  Building wheel for tornado (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tornado: filename=tornado-6.0.3-cp36-cp36m-linux_x86_64.whl size=423197 sha256=5b040e1400746e1a5aeab2e47784bb8f0457a6e97e3db4e1539c33a404e64242\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/bf/40/2f6ef700f48401ca40e5e3dd7d0e3c0a90e064897b7fe5fc08\n",
            "Successfully built alembic colored ConfigArgParse Mako nltk pyrsistent querystring-parser simplejson SQLAlchemy tabulate tornado\n",
            "\u001b[31mERROR: torchvision 0.5.0 has requirement torch==1.4.0, but you'll have torch 1.2.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: kaggle 1.5.6 has requirement urllib3<1.25,>=1.21.1, but you'll have urllib3 1.25.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: jupyter-console 5.2.0 has requirement prompt-toolkit<2.0.0,>=1.0.0, but you'll have prompt-toolkit 2.0.9 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement ipykernel~=4.6.0, but you'll have ipykernel 5.1.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement ipython~=5.5.0, but you'll have ipython 7.7.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement notebook~=5.2.0, but you'll have notebook 6.0.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.21.0, but you'll have requests 2.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement tornado~=4.5.0, but you'll have tornado 6.0.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: SQLAlchemy, Mako, python-editor, python-dateutil, alembic, appnope, attrs, certifi, cloudpickle, colored, ConfigArgParse, configparser, tabulate, urllib3, requests, databricks-cli, ddt, decorator, websocket-client, docker, smmap2, gitdb2, GitPython, gorilla, gunicorn, pyzmq, tornado, traitlets, jupyter-core, jupyter-client, parso, jedi, pexpect, Pygments, wcwidth, prompt-toolkit, ipython, ipykernel, Jinja2, pyrsistent, jsonschema, more-itertools, nbformat, testpath, nbconvert, nltk, terminado, notebook, numpy, scipy, torch, tqdm, openkiwi, pytz, pandas, protobuf, querystring-parser, simplejson, Werkzeug, yamlmagic\n",
            "  Found existing installation: SQLAlchemy 1.3.13\n",
            "    Uninstalling SQLAlchemy-1.3.13:\n",
            "      Successfully uninstalled SQLAlchemy-1.3.13\n",
            "  Found existing installation: python-dateutil 2.6.1\n",
            "    Uninstalling python-dateutil-2.6.1:\n",
            "      Successfully uninstalled python-dateutil-2.6.1\n",
            "  Found existing installation: attrs 19.3.0\n",
            "    Uninstalling attrs-19.3.0:\n",
            "      Successfully uninstalled attrs-19.3.0\n",
            "  Found existing installation: certifi 2019.11.28\n",
            "    Uninstalling certifi-2019.11.28:\n",
            "      Successfully uninstalled certifi-2019.11.28\n",
            "  Found existing installation: cloudpickle 1.2.2\n",
            "    Uninstalling cloudpickle-1.2.2:\n",
            "      Successfully uninstalled cloudpickle-1.2.2\n",
            "  Found existing installation: tabulate 0.8.6\n",
            "    Uninstalling tabulate-0.8.6:\n",
            "      Successfully uninstalled tabulate-0.8.6\n",
            "  Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Found existing installation: requests 2.21.0\n",
            "    Uninstalling requests-2.21.0:\n",
            "      Successfully uninstalled requests-2.21.0\n",
            "  Found existing installation: decorator 4.4.1\n",
            "    Uninstalling decorator-4.4.1:\n",
            "      Successfully uninstalled decorator-4.4.1\n",
            "  Found existing installation: gunicorn 20.0.4\n",
            "    Uninstalling gunicorn-20.0.4:\n",
            "      Successfully uninstalled gunicorn-20.0.4\n",
            "  Found existing installation: pyzmq 17.0.0\n",
            "    Uninstalling pyzmq-17.0.0:\n",
            "      Successfully uninstalled pyzmq-17.0.0\n",
            "  Found existing installation: tornado 4.5.3\n",
            "    Uninstalling tornado-4.5.3:\n",
            "      Successfully uninstalled tornado-4.5.3\n",
            "  Found existing installation: traitlets 4.3.3\n",
            "    Uninstalling traitlets-4.3.3:\n",
            "      Successfully uninstalled traitlets-4.3.3\n",
            "  Found existing installation: jupyter-core 4.6.2\n",
            "    Uninstalling jupyter-core-4.6.2:\n",
            "      Successfully uninstalled jupyter-core-4.6.2\n",
            "  Found existing installation: jupyter-client 5.3.4\n",
            "    Uninstalling jupyter-client-5.3.4:\n",
            "      Successfully uninstalled jupyter-client-5.3.4\n",
            "  Found existing installation: parso 0.6.1\n",
            "    Uninstalling parso-0.6.1:\n",
            "      Successfully uninstalled parso-0.6.1\n",
            "  Found existing installation: jedi 0.16.0\n",
            "    Uninstalling jedi-0.16.0:\n",
            "      Successfully uninstalled jedi-0.16.0\n",
            "  Found existing installation: pexpect 4.8.0\n",
            "    Uninstalling pexpect-4.8.0:\n",
            "      Successfully uninstalled pexpect-4.8.0\n",
            "  Found existing installation: Pygments 2.1.3\n",
            "    Uninstalling Pygments-2.1.3:\n",
            "      Successfully uninstalled Pygments-2.1.3\n",
            "  Found existing installation: wcwidth 0.1.8\n",
            "    Uninstalling wcwidth-0.1.8:\n",
            "      Successfully uninstalled wcwidth-0.1.8\n",
            "  Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "  Found existing installation: ipykernel 4.6.1\n",
            "    Uninstalling ipykernel-4.6.1:\n",
            "      Successfully uninstalled ipykernel-4.6.1\n",
            "  Found existing installation: Jinja2 2.11.1\n",
            "    Uninstalling Jinja2-2.11.1:\n",
            "      Successfully uninstalled Jinja2-2.11.1\n",
            "  Found existing installation: pyrsistent 0.15.7\n",
            "    Uninstalling pyrsistent-0.15.7:\n",
            "      Successfully uninstalled pyrsistent-0.15.7\n",
            "  Found existing installation: jsonschema 2.6.0\n",
            "    Uninstalling jsonschema-2.6.0:\n",
            "      Successfully uninstalled jsonschema-2.6.0\n",
            "  Found existing installation: more-itertools 8.2.0\n",
            "    Uninstalling more-itertools-8.2.0:\n",
            "      Successfully uninstalled more-itertools-8.2.0\n",
            "  Found existing installation: nbformat 5.0.4\n",
            "    Uninstalling nbformat-5.0.4:\n",
            "      Successfully uninstalled nbformat-5.0.4\n",
            "  Found existing installation: testpath 0.4.4\n",
            "    Uninstalling testpath-0.4.4:\n",
            "      Successfully uninstalled testpath-0.4.4\n",
            "  Found existing installation: nbconvert 5.6.1\n",
            "    Uninstalling nbconvert-5.6.1:\n",
            "      Successfully uninstalled nbconvert-5.6.1\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "  Found existing installation: terminado 0.8.3\n",
            "    Uninstalling terminado-0.8.3:\n",
            "      Successfully uninstalled terminado-0.8.3\n",
            "  Found existing installation: notebook 5.2.2\n",
            "    Uninstalling notebook-5.2.2:\n",
            "      Successfully uninstalled notebook-5.2.2\n",
            "  Found existing installation: numpy 1.17.5\n",
            "    Uninstalling numpy-1.17.5:\n",
            "      Successfully uninstalled numpy-1.17.5\n",
            "  Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Found existing installation: torch 1.4.0\n",
            "    Uninstalling torch-1.4.0:\n",
            "      Successfully uninstalled torch-1.4.0\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "  Found existing installation: pytz 2018.9\n",
            "    Uninstalling pytz-2018.9:\n",
            "      Successfully uninstalled pytz-2018.9\n",
            "  Found existing installation: pandas 0.25.3\n",
            "    Uninstalling pandas-0.25.3:\n",
            "      Successfully uninstalled pandas-0.25.3\n",
            "  Found existing installation: protobuf 3.10.0\n",
            "    Uninstalling protobuf-3.10.0:\n",
            "      Successfully uninstalled protobuf-3.10.0\n",
            "  Found existing installation: Werkzeug 1.0.0\n",
            "    Uninstalling Werkzeug-1.0.0:\n",
            "      Successfully uninstalled Werkzeug-1.0.0\n",
            "Successfully installed ConfigArgParse-0.14.0 GitPython-3.0.1 Jinja2-2.10.1 Mako-1.1.0 Pygments-2.4.2 SQLAlchemy-1.3.7 Werkzeug-0.15.5 alembic-1.0.11 appnope-0.1.0 attrs-19.1.0 certifi-2019.6.16 cloudpickle-1.2.1 colored-1.3.93 configparser-3.8.1 databricks-cli-0.8.7 ddt-1.2.1 decorator-4.4.0 docker-4.0.2 gitdb2-2.0.5 gorilla-0.3.0 gunicorn-19.9.0 ipykernel-5.1.2 ipython-7.7.0 jedi-0.15.1 jsonschema-3.0.2 jupyter-client-5.3.1 jupyter-core-4.5.0 more-itertools-5.0.0 nbconvert-5.6.0 nbformat-4.4.0 nltk-3.4.5 notebook-6.0.1 numpy-1.17.0 openkiwi-0.1.2 pandas-0.25.0 parso-0.5.1 pexpect-4.7.0 prompt-toolkit-2.0.9 protobuf-3.9.1 pyrsistent-0.15.4 python-dateutil-2.8.0 python-editor-1.0.4 pytz-2019.2 pyzmq-18.1.0 querystring-parser-1.2.4 requests-2.22.0 scipy-1.3.1 simplejson-3.16.0 smmap2-2.0.5 tabulate-0.8.3 terminado-0.8.2 testpath-0.4.2 torch-1.2.0 tornado-6.0.3 tqdm-4.34.0 traitlets-4.3.2 urllib3-1.25.3 wcwidth-0.1.7 websocket-client-0.56.0 yamlmagic-0.2.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "certifi",
                  "cloudpickle",
                  "dateutil",
                  "decorator",
                  "google",
                  "ipykernel",
                  "jupyter_client",
                  "jupyter_core",
                  "numpy",
                  "pandas",
                  "pexpect",
                  "prompt_toolkit",
                  "pygments",
                  "pytz",
                  "requests",
                  "scipy",
                  "tornado",
                  "tqdm",
                  "traitlets",
                  "urllib3",
                  "wcwidth",
                  "zmq"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMwGu7R0Y33T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# All imports\n",
        "\n",
        "# !git clone https://github.com/facebookresearch/fastText.git\n",
        "# !pip install ./fastText/.\n",
        "# import fasttext\n",
        "# import fasttext.util\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "from nltk import download\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity as sim\n",
        "\n",
        "import pickle\n",
        "\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "import tqdm\n",
        "\n",
        "import kiwi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8aHBVbcSKQ1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "88ce2078-adc3-4569-9330-50676a2a07ee"
      },
      "source": [
        "# Get OpenKiwi model\n",
        "if not exists('estimator_en_de.zip'):\n",
        "    !wget -o estimator_en_de.zip 'https://github.com/unbabel/KiwiCutter/releases/download/v1.0/estimator_en_de.torch.zip'\n",
        "    !unzip estimator_en_de.torch\n",
        "model_kiwi = kiwi.load_model('estimator_en_de.torch')\n",
        "\n",
        "model_kiwi.predict({'source': ['I like to hike in the mountains'],\n",
        "                    'target':['Ich wandere gerne in den Bergen']})"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  estimator_en_de.torch.zip\n",
            "  inflating: estimator_en_de.torch   \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'gap_tags': [[0.5304548740386963,\n",
              "   0.0649648904800415,\n",
              "   0.0730169266462326,\n",
              "   0.08218620717525482,\n",
              "   0.03723501041531563,\n",
              "   0.15140247344970703,\n",
              "   0.02526249922811985]],\n",
              " 'sentence_scores': [0.6461204886436462],\n",
              " 'tags': [[0.9378803372383118,\n",
              "   0.9655714631080627,\n",
              "   0.9754980206489563,\n",
              "   0.9426648616790771,\n",
              "   0.9505487680435181,\n",
              "   0.9422469735145569]]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxB92Q8LotmW",
        "colab_type": "text"
      },
      "source": [
        "# **2. Preprocessing and Feature Extraction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucSsW-VrzluM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download and load different kinds of embeddings\n",
        "class Embedding:\n",
        "  def __init__(self):\n",
        "    self.ft = None\n",
        "    self.ft_de = None\n",
        "    self.nlp_de = None\n",
        "    self.nlp_en = None\n",
        "    self.wvecs = None\n",
        "    self.german_wvecs = None\n",
        "    # stopwords dictionary, run once\n",
        "    download('stopwords')\n",
        "    self.stop_words_en = set(stopwords.words('english'))\n",
        "    self.stop_words_de = set(stopwords.words('german'))\n",
        "\n",
        "  def download_fast_text(self):\n",
        "    !wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
        "    !gunzip cc.en.300.bin.gz\n",
        "    !wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.de.300.bin.gz\n",
        "    !gunzip cc.de.300.bin.gz\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # FastText models are too large, hence reduce the model dimentions to 100\n",
        "    self.ft = fasttext.load_model('cc.en.300.bin')\n",
        "    self.ft_de = fasttext.load_model('cc.de.300.bin')\n",
        "    fasttext.util.reduce_model(self.ft, 100)\n",
        "    self.ft.save_model('/drive/My Drive/cc.en.100.bin')\n",
        "\n",
        "    fasttext.util.reduce_model(self.ft_de, 100)\n",
        "    self.ft.save_model('drive/My Drive/cc.de.100.bin')\n",
        "\n",
        "  def load_fast_text(self):\n",
        "    self.ft_en = fasttext.load_model('drive/My Drive/cc.de.100.bin')\n",
        "    self.ft_de = fasttext.load_model('drive/My Drive/cc.en.100.bin')\n",
        "\n",
        "  def load_spacy(self):\n",
        "    !spacy download en_core_web_md\n",
        "    !spacy link en_core_web_md en300\n",
        "\n",
        "    !spacy download de_core_news_md\n",
        "    !spacy link de_core_news_md de300\n",
        "\n",
        "    self.nlp_de = spacy.load('de300')\n",
        "    self.nlp_en = spacy.load('en300')\n",
        "\n",
        "  def load_muse(self):\n",
        "    !wget https://dl.fbaipublicfiles.com/arrival/vectors/wiki.multi.en.vec\n",
        "    !wget https://dl.fbaipublicfiles.com/arrival/vectors/wiki.multi.de.vec\n",
        "\n",
        "    self.wvecs = {}\n",
        "    with open(\"./wiki.multi.en.vec\", \"r\") as ende_src:\n",
        "      for line in ende_src:\n",
        "        word = line.split(\" \")[0]\n",
        "        vector = [float(a) for a in line.split(\" \")[1:]]\n",
        "        self.wvecs[word] = vector\n",
        "\n",
        "    self.german_wvecs = {}\n",
        "    with open(\"./wiki.multi.de.vec\", \"r\") as ende_src:\n",
        "      for line in ende_src:\n",
        "        word = line.split(\" \")[0]\n",
        "        vector = [float(a) for a in line.split(\" \")[1:]]\n",
        "        self.german_wvecs[word] = vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgSp6q5CyzN1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Helper functions\n",
        "class HelperMethods:\n",
        "\n",
        "  @staticmethod\n",
        "  def get_sentence_emb(line, nlp, lang):\n",
        "    if lang == 'en':\n",
        "      text = line.lower()\n",
        "      l = [token.lemma_ for token in nlp.tokenizer(text)]\n",
        "      l = ' '.join([word for word in l if word not in embedding.stop_words_en])\n",
        "\n",
        "    elif lang == 'de':\n",
        "      text = line.lower()\n",
        "      l = [token.lemma_ for token in nlp.tokenizer(text)]\n",
        "      l = ' '.join([word for word in l if word not in embedding.stop_words_de])\n",
        "\n",
        "    sentence = nlp(l)\n",
        "    return sentence.vector\n",
        "\n",
        "  @staticmethod\n",
        "  def get_sentence_emb_using_word_embs(line, nlp, lang):\n",
        "      if lang == 'en':\n",
        "          text = line.lower()\n",
        "          l = [token.lemma_ for token in nlp.tokenizer(text)]\n",
        "          l = [word for word in l if word not in embedding.stop_words_en]\n",
        "\n",
        "      elif lang == 'de':\n",
        "          text = line.lower()\n",
        "          l = [token.lemma_ for token in nlp.tokenizer(text)]\n",
        "          l = [word for word in l if word not in embedding.stop_words_de]\n",
        "\n",
        "      sentence = []\n",
        "      for word in l:\n",
        "          sentence.append(nlp(word).vector)\n",
        "      return sentence\n",
        "\n",
        "  @staticmethod\n",
        "  def get_tokens(line, lang, nlp):\n",
        "    text = line.lower()\n",
        "    l = [token.lemma_ for token in nlp.tokenizer(text)]\n",
        "    l = ' '.join([word for word in l])\n",
        "    return nlp(l)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qdGPIRTdjO3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Static methods that compute the features for a sentence or a pair of sentencess\n",
        "class FeatureExtraction:\n",
        "  @staticmethod\n",
        "  def get_pos_tag_counts(line, lang, nlp):\n",
        "    pos_counts = {'ADJ': 0,\n",
        "                  'ADP': 0,\n",
        "                  'ADV': 0,\n",
        "                  'AUX': 0,\n",
        "                  'CONJ':\t0,\n",
        "                  'CCONJ':0,\n",
        "                  'DET':\t0,\n",
        "                  'INTJ':\t0,\n",
        "                  'NOUN': 0,\n",
        "                  'NUM': 0,\n",
        "                  'PART':\t0,\n",
        "                  'PRON': 0,\n",
        "                  'PROPN': 0,\n",
        "                  'PUNCT': 0,\n",
        "                  'SCONJ': 0,\n",
        "                  'SYM': 0,\n",
        "                  'VERB': 0,\n",
        "                  'X': 0,\n",
        "                  'SPACE': 0}\n",
        "    \n",
        "    sen = HelperMethods.get_tokens(line, lang, nlp)\n",
        "    for token in sen:\n",
        "      pos_counts[token.pos_] += 1\n",
        "    \n",
        "    return pos_counts\n",
        "\n",
        "  @staticmethod\n",
        "  def get_kiwi_scores(en_sent, de_sent):\n",
        "    scores = model_kiwi.predict({'source': [en_sent], 'target':[de_sent]})\n",
        "    return scores['sentence_scores'][0]\n",
        "\n",
        "  @staticmethod\n",
        "  def get_cosine_sim_with_google_translations(filepath):\n",
        "    file = open(filepath)\n",
        "    lines = file.readlines()\n",
        "    return np.array(lines).astype(float)\n",
        "\n",
        "  @staticmethod\n",
        "  def get_named_entities(line, lang, nlp):\n",
        "    named_entities = {\n",
        "        u'CARDINAL': 0, \n",
        "        u'DATE': 0, \n",
        "        u'EVENT': 0, \n",
        "        u'FAC': 0, \n",
        "        u'GPE': 0, \n",
        "        u'LANGUAGE': 0, \n",
        "        u'LAW': 0, \n",
        "        u'LOC': 0, \n",
        "        u'MONEY': 0, \n",
        "        u'NORP': 0, \n",
        "        u'ORDINAL': 0, \n",
        "        u'ORG': 0, \n",
        "        u'PERCENT': 0, \n",
        "        u'PERSON': 0, \n",
        "        u'PRODUCT': 0, \n",
        "        u'QUANTITY': 0, \n",
        "        u'TIME': 0, \n",
        "        u'WORK_OF_ART': 0, \n",
        "        u'': 0, \n",
        "        u'MISC': 0,\n",
        "        u'PER': 0\n",
        "    }\n",
        "    sen = HelperMethods.get_tokens(line, lang, nlp)\n",
        "    for ent in sen.ents:\n",
        "      if ent.label_ in named_entities:\n",
        "        named_entities[ent.label_] += 1\n",
        "      else:\n",
        "        print(\"Encountered unknown label:\", ent.label_)\n",
        "        named_entities[''] += 1\n",
        "    \n",
        "    return named_entities\n",
        "\n",
        "  @staticmethod\n",
        "  def get_num_of_tokens(sent, lang, nlp):\n",
        "    tokens = HelperMethods.get_tokens(sent, lang, nlp)\n",
        "    return len(tokens)\n",
        "\n",
        "  @staticmethod\n",
        "  def get_avg_source_token_length(sent, nlp_en):\n",
        "    tokens = HelperMethods.get_tokens(sent, 'en', nlp_en)\n",
        "    sum = 0\n",
        "    for token in tokens:\n",
        "      sum += len(token)\n",
        "    return sum / len(tokens)\n",
        "\n",
        "  @staticmethod\n",
        "  def get_num_of_punctuations(sent, nlp):\n",
        "    punctuation = [',','.','...','\\'', '\"', '(', ')', '[', ']']\n",
        "    tokens = HelperMethods.get_tokens(sent, 'en', nlp)\n",
        "    sum = 0\n",
        "    for token in tokens:\n",
        "      if token.text in punctuation:\n",
        "        sum += 1\n",
        "    return sum\n",
        "\n",
        "  @staticmethod\n",
        "  def get_num_of_numberic_tokens(sent, nlp):\n",
        "    tokens = HelperMethods.get_tokens(sent, 'en', nlp)\n",
        "    sum = 0\n",
        "    for token in tokens:\n",
        "      if token.text.isdigit():\n",
        "        sum += 1\n",
        "    return sum\n",
        "\n",
        "  @staticmethod\n",
        "  def get_num_of_alpha_tokens(sent, nlp):\n",
        "    tokens = HelperMethods.get_tokens(sent, 'en', nlp)\n",
        "    sum = 0\n",
        "    for token in tokens:\n",
        "      if token.text.isalpha():\n",
        "        sum += 1\n",
        "    return sum\n",
        "\n",
        "  @staticmethod\n",
        "  def get_ratio_of_target_source_lengths(en_sent, de_sent, nlp_en, nlp_de):\n",
        "    standard_ratio_of_target_source = 219 / 200\n",
        "    num_of_tokens_en = FeatureExtraction.get_num_of_tokens(en_sent, 'en', nlp_en)\n",
        "    num_of_tokens_de = FeatureExtraction.get_num_of_tokens(de_sent, 'de', nlp_de)\n",
        "    return (num_of_tokens_en / num_of_tokens_de) - standard_ratio_of_target_source\n",
        "\n",
        "  @staticmethod\n",
        "  def get_percentage_of_numbers_in_target(sent, nlp):\n",
        "    return FeatureExtraction.get_num_of_numberic_tokens(sent, nlp) / FeatureExtraction.get_num_of_tokens(sent, 'de', nlp)\n",
        "\n",
        "  # Helper method for get_similarities - The Hungarian matching algorithm\n",
        "  @staticmethod\n",
        "  def get_most_matching_words(matrix):\n",
        "    row_ind, col_ind = linear_sum_assignment(matrix)\n",
        "    return row_ind, col_ind\n",
        "\n",
        "  # Helper method for get_similarities\n",
        "  @staticmethod\n",
        "  def find_order(arr):\n",
        "    row_ind, col_ind = get_most_matching_words(arr)\n",
        "    return col_ind\n",
        "\n",
        "  # Get the cosine similarities between two embeddings by matching two lists of words\n",
        "  # using the Hungarian matching algorithm. \n",
        "  @staticmethod\n",
        "  def get_similarities(english_embs, german_embs):\n",
        "    x_vals = []\n",
        "    for idx in tqdm.tqdm(range(len(german_embs))):\n",
        "      arr = None\n",
        "\n",
        "      for i in range(len(german_embs[idx])):\n",
        "        inner_arr = []\n",
        "        for j in range(len(english_embs[idx])):\n",
        "          inner_arr.append(-sim([german_embs[idx][i],english_embs[idx][j]])[0][1])\n",
        "        if arr is None:\n",
        "          arr = np.array([inner_arr])\n",
        "        else:\n",
        "          arr = np.concatenate((arr, [inner_arr]), axis=0)\n",
        "\n",
        "      if arr is None:\n",
        "        x_vals.append([])\n",
        "        continue\n",
        "  \n",
        "      max_length = max(len(german_embs[idx]), len(english_embs[idx]))\n",
        "      blanks = np.zeros((max_length,max_length))\n",
        "      blanks[:arr.shape[0],:arr.shape[1]] = arr\n",
        "      arr = blanks\n",
        "      order = find_order(arr)\n",
        "\n",
        "      vals = []\n",
        "      for i in range(len(order)):\n",
        "        vals.append(arr[i][order[i]])\n",
        "      x_vals.append(np.array(vals))\n",
        "    return x_vals"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoJ6kp6JWxM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Methods that perform preprocessing and convert sentences into embeddings\n",
        "class Preprocessing:\n",
        "  # Pad the sentence embeddings with 0\n",
        "  def pad_sentences(self, embeddings):\n",
        "    pad = 2900\n",
        "    padded_embeddings = []\n",
        "    for i in embeddings:\n",
        "      padded_embeddings.append(np.concatenate((i, ([0] * (pad - len(i)))), axis=0))\n",
        "    return padded_embeddings\n",
        "\n",
        "  # Returns the fastText embeddings of all its sentences given a filepath\n",
        "  def get_fast_text_embeddings(self, f, nlp, stopwords, ftm):\n",
        "    punctuation = [',','.','...','\\'', '\"', '(', ')', '[', ']']\n",
        "    lines_embs = []\n",
        "    file = open(f) \n",
        "    lines = file.readlines()\n",
        "    for l in lines:\n",
        "      text = l.lower()\n",
        "      l = [token.lemma_ for token in nlp.tokenizer(text)]\n",
        "      l = [word for word in l if word not in stopwords]\n",
        "      l = [word for word in l if word not in punctuation]\n",
        "      l = \"\".join([word for word in l])\n",
        "      l = l.rstrip()\n",
        "      lines_embs.append(ftm.get_sentence_vector(l))\n",
        "    return lines_embs\n",
        "\n",
        "  # Returns the SpaCy embeddings of all its sentences given a filepath\n",
        "  def get_spacy_embeddings(self, f, nlp, stopwords):\n",
        "    punctuation = [',','.','...','\\'', '\"', '(', ')', '[', ']']\n",
        "    file = open(f) \n",
        "    lines = file.readlines()\n",
        "    documents = nlp.pipe(lines, batch_size=32, n_threads=7)\n",
        "    lines_embs = []\n",
        "    for doc in documents:\n",
        "      l = []\n",
        "      for token in doc:\n",
        "        if token.text in stopwords or token.text in punctuation:\n",
        "          continue\n",
        "        else:\n",
        "          l.append(token.vector)\n",
        "      lines_embs.append(np.mean(np.array(l), axis=0))\n",
        "    return lines_embs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XX-5RfXknUZc",
        "colab_type": "code",
        "outputId": "9ff33650-6a13-43df-fc58-97fc2d8b7161",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "# Creation of common objects that can be used for any types of model\n",
        "embedding = Embedding()\n",
        "embedding.load_spacy()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_md==2.1.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.1.0/en_core_web_md-2.1.0.tar.gz (95.4MB)\n",
            "\u001b[K     |████████████████████████████████| 95.4MB 1.0MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: en-core-web-md\n",
            "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-md: filename=en_core_web_md-2.1.0-cp36-none-any.whl size=97126236 sha256=a0617070563d0bd43b6867ed972c684058416bd7462b9c22d55e6697139a496e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-z8y4o0cq/wheels/c1/2c/5f/fd7f3ec336bf97b0809c86264d2831c5dfb00fc2e239d1bb01\n",
            "Successfully built en-core-web-md\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_md -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en300\n",
            "You can now load the model via spacy.load('en300')\n",
            "Collecting de_core_news_md==2.1.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_md-2.1.0/de_core_news_md-2.1.0.tar.gz (220.8MB)\n",
            "\u001b[K     |████████████████████████████████| 220.8MB 47.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: de-core-news-md\n",
            "  Building wheel for de-core-news-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for de-core-news-md: filename=de_core_news_md-2.1.0-cp36-none-any.whl size=224546880 sha256=284a1f1370322f17a45f2a2df9e85f6944d99076ad4c54090ecd09d2d22a4999\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xxyophsm/wheels/44/34/f1/31d4b0fa32008c09695ccb180865f196ecd9d512c146f99749\n",
            "Successfully built de-core-news-md\n",
            "Installing collected packages: de-core-news-md\n",
            "Successfully installed de-core-news-md-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_md')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/de_core_news_md -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/de300\n",
            "You can now load the model via spacy.load('de300')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAeI7hHwVLoS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preprocessing = Preprocessing()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qnqpwl4fegE",
        "colab_type": "text"
      },
      "source": [
        "#**3A. Embeddings**\n",
        "\n",
        "This section uses embeddings as input to the diffrenet models in later section. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4wo1kMlkEhp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get English and German FastText Embeddings by passing in arguments:\n",
        "# en_filepath (the English sentences filepath) and de_filepath (the German sentences filepath)\n",
        "def get_fast_text_embeddings(en_filepath, de_filepath):\n",
        "  embedding.download_fast_text()\n",
        "  embedding.load_fast_text()\n",
        "\n",
        "  concatenated_english_embs = preprocessing.get_fast_text_embeddings(en_filepath, \n",
        "                                                        embedding.nlp_en, \n",
        "                                                        embedding.stop_words_en, \n",
        "                                                        embedding.ft)\n",
        "  concatenated_german_embs = preprocessing.get_fast_text_embeddings(de_filepath, \n",
        "                                                       embedding.nlp_de, \n",
        "                                                       embedding.stop_words_de, \n",
        "                                                       embedding.ft_de)\n",
        "\n",
        "  english_embs = []\n",
        "  for e in concatenated_english_embs:\n",
        "    sentence_embs = []\n",
        "    for i in range(0, len(e), 100):\n",
        "      sentence_embs.append(e[i : (i + 100)])\n",
        "    english_embs.append(sentence_embs)\n",
        "\n",
        "  german_embs = []\n",
        "  for e in concatenated_german_embs:\n",
        "    sentence_embs = []\n",
        "    for i in range(0, len(e), 100):\n",
        "      sentence_embs.append(e[i : (i + 100)])\n",
        "    german_embs.append(sentence_embs)\n",
        "  \n",
        "  return english_embs, german_embs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiPgPRRzy_ax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get Spacy Embedding\n",
        "def get_spacy_embeddings(en_filepath, de_filepath):\n",
        "  english_embs = preprocessing.get_spacy_embeddings(en_filepath, embedding.nlp_en, embedding.stop_words_en)\n",
        "  german_embs = preprocessing.get_spacy_embeddings(de_filepath, embedding.nlp_de, embedding.stop_words_de)\n",
        "  return english_embs, german_embs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xa_rdSXdQ_kh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "english_embs, german_embs = get_spacy_embeddings(\"./train.ende.src\", \"./train.ende.mt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqtThqijoHmL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('english_embs.txt', 'wb') as pickle_file:\n",
        "  pickle.dump(english_embs, pickle_file)\n",
        "with open('german_embs.txt', 'wb') as pickle_file:\n",
        "  pickle.dump(german_embs, pickle_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUqBxc9sD1b9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_combined_embeddings(english_emb, german_emb):\n",
        "  features = []\n",
        "  for value in english_emb:\n",
        "    features.append(value)\n",
        "  for value in german_emb:\n",
        "    features.append(value)\n",
        "\n",
        "  return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKAQ1U49EHP8",
        "colab_type": "code",
        "outputId": "74b7be8d-1e11-4ed6-dab0-3802270e1ea6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Appending English and German word embeddings\n",
        "samples = []\n",
        "for i in tqdm.tqdm(range(len(english_embs))):\n",
        "  samples.append(get_combined_embeddings(english_embs[i], german_embs[i]))"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 7000/7000 [00:00<00:00, 11880.87it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuV6nDXVAHxG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('train_features.txt', 'wb') as pickle_file:\n",
        "  pickle.dump(samples, pickle_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhCvHjOFhpZx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "samples_best_features = samples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4F-MeZ6sjg0I",
        "colab_type": "text"
      },
      "source": [
        "#**3B. Features**\n",
        "This section extracts features from the input sentences and feed them as input to the diffrenet models in later section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfQEvFC9llPX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_all_features(line_en, line_de, cosine_sim_with_google_translations):\n",
        "  features = []\n",
        "\n",
        "  english_pos_counts = FeatureExtraction.get_pos_tag_counts(line_en, 'en', embedding.nlp_en)\n",
        "  german_pos_counts = FeatureExtraction.get_pos_tag_counts(line_de, 'de', embedding.nlp_de)\n",
        "  for k in english_pos_counts.keys():\n",
        "    features.append(english_pos_counts[k])\n",
        "\n",
        "  for k in german_pos_counts.keys():\n",
        "    features.append(german_pos_counts[k])\n",
        "\n",
        "  english_named_entities = FeatureExtraction.get_named_entities(line_en, 'en', embedding.nlp_en)\n",
        "  german_named_entities = FeatureExtraction.get_named_entities(line_de, 'de', embedding.nlp_de)\n",
        "  for k in english_named_entities.keys():\n",
        "    features.append(english_named_entities[k])\n",
        "\n",
        "  for k in german_named_entities.keys():\n",
        "    features.append(german_named_entities[k])\n",
        "\n",
        "  features.append(FeatureExtraction.get_kiwi_scores(line_en, line_de))\n",
        "  features.append(FeatureExtraction.get_num_of_tokens(line_en, 'en', embedding.nlp_en))\n",
        "  features.append(FeatureExtraction.get_num_of_tokens(line_de, 'de', embedding.nlp_de))\n",
        "  features.append(FeatureExtraction.get_avg_source_token_length(line_en, embedding.nlp_en))\n",
        "  features.append(FeatureExtraction.get_num_of_punctuations(line_en, embedding.nlp_en))\n",
        "  features.append(FeatureExtraction.get_num_of_punctuations(line_de, embedding.nlp_de))\n",
        "  features.append(FeatureExtraction.get_num_of_numberic_tokens(line_en, embedding.nlp_en))\n",
        "  features.append(FeatureExtraction.get_num_of_numberic_tokens(line_de, embedding.nlp_de))\n",
        "  features.append(FeatureExtraction.get_num_of_alpha_tokens(line_en, embedding.nlp_en))\n",
        "  features.append(FeatureExtraction.get_num_of_alpha_tokens(line_de, embedding.nlp_de))\n",
        "  features.append(FeatureExtraction.get_ratio_of_target_source_lengths(line_en, line_de, embedding.nlp_en, embedding.nlp_de))\n",
        "  features.append(FeatureExtraction.get_percentage_of_numbers_in_target(line_de, embedding.nlp_de))\n",
        "  features.append(cosine_sim_with_google_translations)\n",
        "  \n",
        "  return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvqmYRPfHaUp",
        "colab_type": "code",
        "outputId": "95aea0d7-112a-43e6-d690-0437f17512e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Get all the features for the English and German training sentences\n",
        "\n",
        "file = open('./train.ende.src') \n",
        "lines_en = file.readlines()\n",
        "file = open('./train.ende.mt')\n",
        "lines_de = file.readlines()\n",
        "\n",
        "train_cosine_sim_with_google_translations = FeatureExtraction.get_cosine_sim_with_google_translations('./train_google_translate_api.txt')\n",
        "samples = []\n",
        "for i in tqdm.tqdm(range(len(lines_en))):\n",
        "  samples.append(get_all_features(lines_en[i], lines_de[i], train_cosine_sim_with_google_translations[i]))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 7000/7000 [42:41<00:00,  2.69it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNlIQpkVTKW6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('train_features.txt', 'wb') as pickle_file:\n",
        "  pickle.dump(samples, pickle_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZlgO1yIM6_n",
        "colab_type": "text"
      },
      "source": [
        "## **4. Feature Selection**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KN_oEWSYNAgW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.feature_selection import RFE\n",
        "from itertools import compress\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "from itertools import compress\n",
        "from scipy.stats import pearsonr\n",
        "import sklearn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aA_WfUJ1bGiq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LOADING TRAIN SCORES\n",
        "train_scores_file = open(\"./train.ende.scores\",'r')\n",
        "train_scores = train_scores_file.readlines()\n",
        "\n",
        "file = open('./train_features.txt', 'rb')\n",
        "samples = pickle.load(file)\n",
        "file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoDwb9rTmfcw",
        "colab_type": "text"
      },
      "source": [
        "#**4A. Variance Threshold**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHj-n8_KPGhA",
        "colab_type": "code",
        "outputId": "3f41cdd0-ffee-471b-db9c-9e71303f44dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "sel = VarianceThreshold(threshold=0.5)\n",
        "X_train = sel.fit_transform(samples)\n",
        "features_selection_boolean_vector = sel.get_support()\n",
        "\n",
        "print(\"The number of features left is\", len(X_train[0]))\n",
        "print(\"The feature extraction matrix is\", features_selection_boolean_vector)\n",
        "\n",
        "# Get the best features from the whole set of features\n",
        "samples_best_features = []\n",
        "for sample in samples:\n",
        "  samples_best_features.append(list(compress(sample, features_selection_boolean_vector)))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of features left is 29\n",
            "The feature extraction matrix is [ True  True  True False False  True  True False  True  True False False\n",
            " False  True False False  True False False  True  True  True False  True\n",
            " False  True False  True  True False  True  True  True False False  True\n",
            "  True False False False False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False False False  True  True False\n",
            "  True  True  True  True  True  True False False False]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EW4VWtc0tCeL",
        "colab_type": "text"
      },
      "source": [
        "#**4B. RFE (Recursive Feature Elimination)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1-ejCKXww5_",
        "colab_type": "code",
        "outputId": "fbe278b2-d109-4bfa-b73e-ac4802ccc75e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "# The model for RFE:\n",
        "model = BayesianRidge(n_iter=200, tol=0.001, alpha_1=1e-06, \n",
        "                      alpha_2=1e-06, lambda_1=1e-06, lambda_2=1e-06)\n",
        "# model = LinearRegression(fit_intercept=False)\n",
        "\n",
        "# Initializing RFE model with the number of features to get from RFE\n",
        "number_of_features = 27\n",
        "rfe = RFE(model, number_of_features)\n",
        "\n",
        "# Transforming data using RFE\n",
        "rfe.fit_transform(samples, train_scores)\n",
        "features_selection_boolean_vector = rfe.support_\n",
        "\n",
        "# Get the best features from the whole set of features\n",
        "samples_best_features = []\n",
        "for sample in samples:\n",
        "  samples_best_features.append(list(compress(sample, features_selection_boolean_vector)))\n",
        "\n",
        "print(\"The number of features left is\", number_of_features)\n",
        "print(\"The feature extraction matrix is\", features_selection_boolean_vector)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of features left is 27\n",
            "The feature extraction matrix is [False False False False False False  True  True False False  True False\n",
            " False False False False False  True False False False False  True  True\n",
            " False False False  True  True  True False False False False False False\n",
            "  True False  True  True False False False False False False False  True\n",
            "  True  True False  True False  True False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False  True False False False  True  True\n",
            "  True  True  True  True  True  True False False  True]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1cn4JmMv76B",
        "colab_type": "text"
      },
      "source": [
        "#**4C. No feature selection**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jweMmX74wH-T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features_selection_boolean_vector = [True] * (len(samples[0]))\n",
        "\n",
        "samples_best_features = []\n",
        "for sample in samples:\n",
        "  samples_best_features.append(list(compress(sample, features_selection_boolean_vector)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbRrpy4ytdsz",
        "colab_type": "text"
      },
      "source": [
        "#**5. Load Validation Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cnx80dZoTmZH",
        "colab_type": "code",
        "outputId": "87e69b38-cccd-40b7-a170-ddceb291d69c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Run if using features, get all the features for the English and German validation sentences\n",
        "# IMPORTANT: Skip this cell if val_samples has been saved to a text file before\n",
        "val_src_file = open('./dev.ende.src') \n",
        "val_lines_en = val_src_file.readlines()\n",
        "val_mt_file = open('./dev.ende.mt')\n",
        "val_lines_de = val_mt_file.readlines()\n",
        "\n",
        "val_cosine_sim_with_google_translations = FeatureExtraction.get_cosine_sim_with_google_translations('./val_google_translate_api.txt')\n",
        "val_samples = []\n",
        "for i in tqdm.tqdm(range(len(val_lines_en))):\n",
        "    val_samples.append(get_all_features(val_lines_en[i], val_lines_de[i], val_cosine_sim_with_google_translations[i]))\n",
        "\n",
        "with open('val_features.txt', 'wb') as pickle_file:\n",
        "  pickle.dump(val_samples, pickle_file)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [06:03<00:00,  2.72it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYaM9DvDoxvv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run if using features, load features of validation sentences from a text file\n",
        "file = open('./val_features.txt', 'rb')\n",
        "val_samples = pickle.load(file)\n",
        "file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kn8Lve62oqYr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run if using features, retrieve the best subset of features from the full feature set\n",
        "val_samples_best_features = []\n",
        "for val_sample in val_samples:\n",
        "  val_samples_best_features.append(list(compress(val_sample, features_selection_boolean_vector)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZtPBx0bgbIx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run if using embeddings, get the embeddings of validation senteces\n",
        "val_english_embs, val_german_embs = get_spacy_embeddings(\"./dev.ende.src\", \"./dev.ende.mt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNsrMdKlgyWs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d63a054b-e442-4e45-87e9-4f8b71b0ecf5"
      },
      "source": [
        "# Run if using embeddings, appending English and German validation word embeddings\n",
        "val_samples_best_features = []\n",
        "for i in tqdm.tqdm(range(len(val_english_embs))):\n",
        "  val_samples_best_features.append(get_combined_embeddings(val_english_embs[i], val_german_embs[i]))"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:00<00:00, 11801.58it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uK21vKw-Trq5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read the validation scores\n",
        "val_scores_file = open(\"./dev.ende.scores\",'r')\n",
        "val_scores = val_scores_file.readlines()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uF6cwybiznfQ",
        "colab_type": "text"
      },
      "source": [
        "#**6A. Training on Linear Regression Model and Evaluation on Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfK_YmHNqw6m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit the input model with x_train and y_train, and then predict with x_test and\n",
        "# compute the pearson_correlation, mae and mse with the y_test\n",
        "def get_metrics(x_train, y_train, x_test, y_test, model):\n",
        "  model.fit(x_train, y_train)\n",
        "  y_hat = model.predict(x_test)\n",
        "  mse = sklearn.metrics.mean_squared_error(y_hat, y_test)\n",
        "  mae = sklearn.metrics.mean_absolute_error(y_hat, y_test)\n",
        "  pearson_cor, p_value = pearsonr(np.array(y_hat.flatten()), y_test.astype(float))\n",
        "\n",
        "  return pearson_cor, mae, mse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZVAB4TUYSfg",
        "colab_type": "code",
        "outputId": "90cc934a-bc14-4718-c1b0-d1328bc054c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        }
      },
      "source": [
        "# The estimators that will be executed\n",
        "highest_pearson = 0\n",
        "best_linear_model = None\n",
        "estimators = [('Support Vector Machine Regressor', SVR(C=10, epsilon=0.01, gamma=0.0001)),\n",
        "              ('Bayesian Ridge', BayesianRidge(n_iter=200, tol=0.001, alpha_1=1e-06, alpha_2=1e-06, lambda_1=1e-06, lambda_2=1e-06)),\n",
        "              ('Random Forest Regressor', RandomForestRegressor()),\n",
        "              ('Pylonomial Features with Linear Regression', make_pipeline(PolynomialFeatures(1), LinearRegression(fit_intercept=False))),\n",
        "              ('Ridge Regressor', sklearn.linear_model.Ridge()),\n",
        "              ('Orthogonal Matching', sklearn.linear_model.OrthogonalMatchingPursuit(n_nonzero_coefs=1)),\n",
        "              ('Ridge', sklearn.linear_model.Ridge(alpha=.5))]\n",
        "\n",
        "for name, model in estimators:\n",
        "  pearson_cor, mae, mse = get_metrics(np.array(samples_best_features), np.array(train_scores), \n",
        "                                     np.array(val_samples_best_features), np.array(val_scores).astype(float), model)\n",
        "  print(f'[%s] MSE: %.4f MAE %.4f with peason correlation of %0.4f' % (name, mse, mae, pearson_cor))\n",
        "  if pearson_cor > highest_pearson:\n",
        "    highest_pearson = pearson_cor\n",
        "    best_linear_model = model"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Support Vector Machine Regressor] MSE: 0.7662 MAE 0.4876 with peason correlation of 0.1446\n",
            "[Bayesian Ridge] MSE: 0.7311 MAE 0.5219 with peason correlation of 0.1572\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-154-c4c6c41a0e6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mestimators\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   pearson_cor, mae, mse = get_metrics(np.array(samples_best_features), np.array(train_scores), \n\u001b[0;32m---> 13\u001b[0;31m                                      np.array(val_samples_best_features), np.array(val_scores).astype(float), model)\n\u001b[0m\u001b[1;32m     14\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'[%s] MSE: %.4f MAE %.4f with peason correlation of %0.4f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpearson_cor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpearson_cor\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mhighest_pearson\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-153-5f9b6b147817>\u001b[0m in \u001b[0;36mget_metrics\u001b[0;34m(x_train, y_train, x_test, y_test, model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mmae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    381\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                     n_samples_bootstrap=n_samples_bootstrap)\n\u001b[0;32m--> 383\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    833\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1223\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1225\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1226\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    365\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SytK9cBPdPH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf = best_linear_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LQO9_js3fzc",
        "colab_type": "text"
      },
      "source": [
        "#**6B. Training on Neural Networks and Evaluation on Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-kf0tbZ7ib5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from tensorflow import keras\n",
        "from keras import optimizers\n",
        "from keras.layers import Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras import layers\n",
        "from numpy.random import seed\n",
        "\n",
        "import tensorflow as tf\n",
        "from scipy.stats import pearsonr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kajfawum7xPF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "b4dfc394-2d0d-4d96-91ba-e2229b0dd423"
      },
      "source": [
        "layers1 = [16]\n",
        "layers2 = [8]\n",
        "layers3 = [4]\n",
        "layers4 = [8]\n",
        "highest_pearson = 0\n",
        "best_model = None\n",
        "\n",
        "es=EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
        "\n",
        "for layer1 in layers1:\n",
        "  for layer2 in layers2:\n",
        "    for layer3 in layers3:\n",
        "      for layer4 in layers4:\n",
        "        model = Sequential()\n",
        "        model.add(Dense(layer1, input_dim=27, activation='relu'))\n",
        "        # model.add(Dropout(0.2))\n",
        "        model.add(Dense(layer2, activation='relu'))\n",
        "        # model.add(Dropout(0.2))\n",
        "        model.add(Dense(layer3, activation='relu'))\n",
        "        model.add(Dense(layer4, activation='relu'))\n",
        "        model.add(Dense(1))\n",
        "        \n",
        "        # optimizer = optimizers.RMSprop(0.001)\n",
        "        \n",
        "        model.compile(loss='mse',\n",
        "                      optimizer='Adam',\n",
        "                      metrics=['mae', 'mse'])\n",
        "        \n",
        "        model.fit(np.array(samples_best_features), \n",
        "                  np.array(train_scores),\n",
        "                  epochs=50,\n",
        "                  validation_data=(np.array(val_samples_best_features[:500]), np.array(val_scores[:500]).astype(float)),\n",
        "                  callbacks=[es], \n",
        "                  verbose=1)\n",
        "\n",
        "        predictions_validation = model.predict(np.array(val_samples_best_features[500:]))\n",
        "        y_hat = np.array(predictions_validation.flatten())\n",
        "        y_test = np.array(val_scores[500:]).astype(float)\n",
        "\n",
        "        peason_cor, p_value = pearsonr(y_hat, y_test)\n",
        "        mse = sklearn.metrics.mean_squared_error(y_hat, y_test)\n",
        "        mae = sklearn.metrics.mean_absolute_error(y_hat, y_test)\n",
        "        if peason_cor > highest_pearson:\n",
        "          highest_pearson = peason_cor\n",
        "          best_model = model\n",
        "        print(\"The models are: [%f, %f, %f, %f]\" % (layer1, layer2, layer3, layer4))\n",
        "        print(\"Pearson Correlation: %f, MSE: %f, MAE: %f\" % (peason_cor, mse, mae))"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 7000 samples, validate on 500 samples\n",
            "Epoch 1/50\n",
            "7000/7000 [==============================] - 7s 976us/step - loss: 0.6793 - mean_absolute_error: 0.4851 - mean_squared_error: 0.6793 - val_loss: 0.8864 - val_mean_absolute_error: 0.6037 - val_mean_squared_error: 0.8864\n",
            "Epoch 2/50\n",
            "7000/7000 [==============================] - 1s 120us/step - loss: 0.6756 - mean_absolute_error: 0.4826 - mean_squared_error: 0.6756 - val_loss: 0.8808 - val_mean_absolute_error: 0.5786 - val_mean_squared_error: 0.8808\n",
            "Epoch 3/50\n",
            "7000/7000 [==============================] - 1s 112us/step - loss: 0.6698 - mean_absolute_error: 0.4831 - mean_squared_error: 0.6698 - val_loss: 0.8721 - val_mean_absolute_error: 0.5634 - val_mean_squared_error: 0.8721\n",
            "Epoch 4/50\n",
            "7000/7000 [==============================] - 1s 112us/step - loss: 0.6618 - mean_absolute_error: 0.4808 - mean_squared_error: 0.6618 - val_loss: 0.8878 - val_mean_absolute_error: 0.5619 - val_mean_squared_error: 0.8878\n",
            "Epoch 00004: early stopping\n",
            "The models are: [16.000000, 8.000000, 4.000000, 8.000000]\n",
            "Pearson Correlation: 0.088894, MSE: 0.596441, MAE: 0.452051\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjDq1JzI72Ph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf = best_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQ9YrfBr3KN1",
        "colab_type": "text"
      },
      "source": [
        "#**7. Load test dataset and predict test dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOagQEG6crNS",
        "colab_type": "code",
        "outputId": "e3fa25a3-7be4-41cd-bd2e-75c746457c33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        }
      },
      "source": [
        "# Get all the features for the English and German test sentences\n",
        "# IMPORTANT: Skip this cell if test_samples has been saved to a text file before\n",
        "file = open('./test.ende.src') \n",
        "lines_en = file.readlines()\n",
        "file = open('./test.ende.mt')\n",
        "lines_de = file.readlines()\n",
        "\n",
        "test_cosine_sim_with_google_translations = FeatureExtraction.get_cosine_sim_with_google_translations('./test_google_translate_api.txt')\n",
        "\n",
        "test_samples = []\n",
        "for i in tqdm.tqdm(range(len(lines_en))):\n",
        "  test_samples.append(get_all_features(lines_en[i], lines_de[i], test_cosine_sim_with_google_translations[i]))\n",
        "\n",
        "with open('test_features.txt', 'wb') as pickle_file:\n",
        "  pickle.dump(test_samples, pickle_file)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  1%|          | 8/1000 [00:02<05:45,  2.87it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-89-936155f5e096>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtest_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines_en\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mtest_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_all_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines_en\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines_de\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_cosine_sim_with_google_translations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_features.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpickle_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-59-b07ba5b7dae3>\u001b[0m in \u001b[0;36mget_all_features\u001b[0;34m(line_en, line_de, cosine_sim_with_google_translations)\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFeatureExtraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_num_of_numberic_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_de\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp_de\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFeatureExtraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_num_of_alpha_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_en\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp_en\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m   \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFeatureExtraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_num_of_alpha_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_de\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp_de\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m   \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFeatureExtraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ratio_of_target_source_lengths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_en\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline_de\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp_en\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp_de\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFeatureExtraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_percentage_of_numbers_in_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_de\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp_de\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-49-1b36ad271edd>\u001b[0m in \u001b[0;36mget_num_of_alpha_tokens\u001b[0;34m(sent, nlp)\u001b[0m\n\u001b[1;32m    108\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_num_of_alpha_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHelperMethods\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'en'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0msum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-2577d5362721>\u001b[0m in \u001b[0;36mget_tokens\u001b[0;34m(line, lang, nlp)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__call__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE003\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.__call__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.predict\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.greedy_parse\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \"\"\"\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m_parser_model.pyx\u001b[0m in \u001b[0;36mspacy.syntax._parser_model.ParserModel.begin_update\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m_parser_model.pyx\u001b[0m in \u001b[0;36mspacy.syntax._parser_model.ParserStepModel.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/neural/_classes/feed_forward.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/api.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(seqs_in, drop)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseqs_in\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbp_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbp_layer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/neural/_classes/feed_forward.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/neural/_classes/resnet.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbp_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/neural/_classes/feed_forward.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/neural/_classes/layernorm.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mXhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackprop_rescale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_begin_update_scale_shift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXhat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfinish_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/neural/_classes/layernorm.py\u001b[0m in \u001b[0;36m_begin_update_scale_shift\u001b[0;34m(self, input__BI)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgradient__BI\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0minput__BI\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mG\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinish_update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfeIs3v79bIo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load features of test sentences from a text file\n",
        "file = open('./test_features.txt', 'rb')\n",
        "test_samples = pickle.load(file)\n",
        "file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXQTgUit88wR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Retrieve the best subset of features from the full feature set\n",
        "test_samples_best_features = []\n",
        "for test_sample in test_samples:\n",
        "  test_samples_best_features.append(list(compress(test_sample, features_selection_boolean_vector)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOoizRardERu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "01bd71fd-3e79-4dcc-9ddf-52241b8f6860"
      },
      "source": [
        "test_best_features_predictions = clf.predict(np.array(test_samples_best_features))"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<keras.engine.sequential.Sequential object at 0x7f307cf389b0>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4DoOXOMdIa3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def write_test_predications(test_predictions):\n",
        "  f = open(\"predictions.txt\", \"w\")\n",
        "  for num in test_predictions:\n",
        "    f.write(f\"{num[0]}\\n\")\n",
        "  f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RrLtruIdOwc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "write_test_predications(test_best_features_predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h76geFb-dsdj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "zipObj = ZipFile('predictions.txt.zip', 'w')\n",
        " \n",
        "# Add multiple files to the zip\n",
        "zipObj.write('predictions.txt')\n",
        " \n",
        "# close the Zip File\n",
        "zipObj.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQd6gl4YVZL3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}