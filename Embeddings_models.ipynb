{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cx1PC5guOMN1"
   },
   "source": [
    "## 1. Downloads and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "colab_type": "code",
    "id": "nmZCgch8MeXO",
    "outputId": "dbb57aca-b1f7-4605-d1ac-6e03f4324539",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "if not exists('ende_data.zip'):\n",
    "    !wget -O ende_data.zip https://competitions.codalab.org/my/datasets/download/c748d2c0-d6be-4e36-9f12-ca0e88819c4d\n",
    "    !unzip ende_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "colab_type": "code",
    "id": "wiO5vIoTMjb8",
    "outputId": "29c4967f-9a1c-4827-fb98-fe320c120e39"
   },
   "outputs": [],
   "source": [
    "!spacy download en_core_web_md\n",
    "!spacy link en_core_web_md en300\n",
    "\n",
    "!spacy download de_core_news_md\n",
    "!spacy link de_core_news_md de300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O0V-iLkMMnam"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mk4zDCBVMp9K"
   },
   "outputs": [],
   "source": [
    "nlp_de =spacy.load('de300')\n",
    "nlp_en =spacy.load('en300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "MhXFm4zMMs9j",
    "outputId": "7dc47627-4575-4f3a-fd54-a0d5f652fd86"
   },
   "outputs": [],
   "source": [
    "from nltk import download\n",
    "from nltk.corpus import stopwords\n",
    "download('stopwords') #stopwords dictionary, run once\n",
    "\n",
    "stop_words_en = set(stopwords.words('english'))\n",
    "stop_words_de = set(stopwords.words('german'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "id": "ugT1KZ8bmN2A",
    "outputId": "d7e655c3-35e8-4473-ca2c-d76b3132d473"
   },
   "outputs": [],
   "source": [
    "!pip install keras_self_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "yEIiKN1NZaZT",
    "outputId": "ebe005bd-cf6b-4ff6-cc81-67374180dbd6"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, GRU, GlobalMaxPooling1D, SpatialDropout1D, Bidirectional, Flatten, Input, Concatenate\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qCqhqKbdZykn"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8bZpofILOw_n"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_MpVnRoWSTIb"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jMqxTqriTSr-"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hmdMx1bXU2CK"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hFNfzv-PV-fk"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TmcFRhY_O4Pf"
   },
   "source": [
    "## 2. Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MKbFFMY9MvA7"
   },
   "outputs": [],
   "source": [
    "def get_embeddings(lines, nlp, stopwords, lang):\n",
    "  unknown = nlp.vocab['unk'].vector\n",
    "  punctuation = [',','.','...','\\'', '\"', '(', ')', '[', ']']\n",
    "  lines_embs = []\n",
    "  \n",
    "  documents = nlp.pipe(lines, batch_size=32, n_threads=7)\n",
    "  for doc in documents:\n",
    "    l = []\n",
    "    for token in doc:\n",
    "      if token.text in stopwords or token.text in punctuation:\n",
    "        continue\n",
    "      if not token.has_vector:\n",
    "        l.append(unknown)\n",
    "      else:\n",
    "        l.append(token.vector)\n",
    "    lines_embs.append(l)\n",
    "  return lines_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "juczioIOjCk6"
   },
   "outputs": [],
   "source": [
    "def pad_sent(lst):\n",
    "    pad = 35 # maximum sentence length for train, validation and test data\n",
    "    arr = []\n",
    "    for i in lst:\n",
    "      arr.append(np.concatenate((i, ([np.zeros(300)] * (pad-len(i)))), axis=0))\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S-M9CYmL1c8z"
   },
   "outputs": [],
   "source": [
    "# Converts scores file to list of floats\n",
    "def get_scores(f):\n",
    "  scores = open(f, 'r').readlines()\n",
    "  for i in range(len(scores)):\n",
    "    scores[i] = float(scores[i])\n",
    "  return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9_LFQtYztIXe"
   },
   "source": [
    "## 3. Shuffling and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZR910CQjU8pL"
   },
   "outputs": [],
   "source": [
    "# Combines training and validation data, shuffles and splits to 8000 and 1000\n",
    "def shuffle_and_split():\n",
    "  train_f_en = open('./train.ende.src')\n",
    "  lines_train_en = train_f_en.readlines()\n",
    "  train_f_de = open('./train.ende.mt')\n",
    "  lines_train_de = train_f_de.readlines()\n",
    "  val_f_en = open('./dev.ende.src')\n",
    "  lines_val_en = val_f_en.readlines()\n",
    "  val_f_de = open('./dev.ende.mt')\n",
    "  lines_val_de = val_f_de.readlines()\n",
    "\n",
    "  data = []\n",
    "  for i in range(len(lines_train_en)):\n",
    "    data.append((lines_train_en[i], lines_train_de[i]))\n",
    "  \n",
    "  for i in range(len(lines_val_en)):\n",
    "    data.append((lines_val_en[i], lines_val_de[i]))\n",
    "\n",
    "  scores = get_scores('./train.ende.scores')\n",
    "  scores = scores + get_scores('./dev.ende.scores')\n",
    "\n",
    "  X_train, X_val, y_train, y_val = train_test_split(data, scores, train_size=0.875, random_state=42, shuffle=True)\n",
    "\n",
    "  return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r_ZkbrxVX55c"
   },
   "outputs": [],
   "source": [
    "# Gets training and validation splits\n",
    "X_train, X_val, y_train, y_val = shuffle_and_split()\n",
    "english_train = [x for (x, _) in X_train]\n",
    "german_train = [y for (_, y) in X_train]\n",
    "english_val = [x for (x, _) in X_val]\n",
    "german_val = [y for (_, y) in X_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RqIdGUF3PncQ"
   },
   "source": [
    "## 4. Sentence averages to MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-W2Fj1SByTGW"
   },
   "source": [
    "Get embeddings and pad training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IEugaxW4M4po"
   },
   "outputs": [],
   "source": [
    "english_embs = get_embeddings(english_train, nlp_en, stop_words_en, 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4EsfqDjklLZA"
   },
   "outputs": [],
   "source": [
    "english_embs = pad_sent(english_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ci1n7f1_M5Ii"
   },
   "outputs": [],
   "source": [
    "german_embs = get_embeddings(german_train, nlp_de, stop_words_de, 'de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AYeivJdFmIpa"
   },
   "outputs": [],
   "source": [
    "german_embs = pad_sent(german_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QhNafoaignGE"
   },
   "outputs": [],
   "source": [
    "for i in range(len(english_embs)):\n",
    "  english_embs[i] = np.array(english_embs[i]).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v6Ea7oZZp8KM"
   },
   "outputs": [],
   "source": [
    "for i in range(len(german_embs)):\n",
    "  german_embs[i] = np.array(german_embs[i]).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2gooE88NY7yM"
   },
   "outputs": [],
   "source": [
    "english_embs2 = get_embeddings(english_val, nlp_en, stop_words_en, 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_UYMRFX7ZAMY"
   },
   "outputs": [],
   "source": [
    "german_embs2 = get_embeddings(german_val, nlp_de, stop_words_de, 'de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qw_z5m_NqH0h"
   },
   "outputs": [],
   "source": [
    "english_embs2 = pad_sent(english_embs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YSehyWNFqLmj"
   },
   "outputs": [],
   "source": [
    "german_embs2 = pad_sent(german_embs2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sdcJQ6jtyaJS"
   },
   "source": [
    "Find averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CHwRx7ycqQw2"
   },
   "outputs": [],
   "source": [
    "for i in range(len(german_embs2)):\n",
    "  german_embs2[i] = np.array(german_embs2[i]).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p3ZAL5vuqUqR"
   },
   "outputs": [],
   "source": [
    "for i in range(len(english_embs2)):\n",
    "  english_embs2[i] = np.array(english_embs2[i]).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uPlVYkaQqn9h"
   },
   "outputs": [],
   "source": [
    "X_train = []\n",
    "for i in range(len(english_embs)):\n",
    "  X_train.append(np.concatenate((np.array(english_embs[i]), np.array(german_embs[i]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "447NClThrqWA"
   },
   "outputs": [],
   "source": [
    "X_val = []\n",
    "for i in range(len(english_embs2)):\n",
    "  X_val.append(np.concatenate((np.array(english_embs2[i]), np.array(german_embs2[i]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZbMCFjUCPywc"
   },
   "source": [
    "MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "MJL1KLqKZPeN",
    "outputId": "12ac28ab-15fe-42fb-f767-4add3ea638f6"
   },
   "outputs": [],
   "source": [
    "m = Sequential()\n",
    "m.add(Dense(64, activation='relu', input_dim=600))\n",
    "m.add(Dense(128, activation='relu', input_dim=600))\n",
    "m.add(Dense(64, activation='relu', input_dim=600))\n",
    "m.add(Dense(1))\n",
    "m.summary()\n",
    "m.compile(loss='mse',\n",
    "    optimizer='Adam',\n",
    "    metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "colab_type": "code",
    "id": "PsaiCrAwZW3A",
    "outputId": "6266edef-6447-42f2-bf96-8346e37d9f44"
   },
   "outputs": [],
   "source": [
    "m.fit(np.array(X_train),np.array(y_train), epochs=10, validation_data=(np.array(X_val), y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zX_pea0Y36al",
    "outputId": "8d011141-8def-4cc2-f071-0d31fbfe6407"
   },
   "outputs": [],
   "source": [
    "pearson_score, _ = pearsonr(m.predict(np.array(X_val)).squeeze(), y_val)\n",
    "print(\"Pearson score: \", pearson_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1PhoWdE8RNF_"
   },
   "source": [
    "## 5. Parameter tuning with cross validation for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nGnOuXKSRQce"
   },
   "outputs": [],
   "source": [
    "def param_tuning():\n",
    "  lstm_units = [32, 64, 128]\n",
    "  lstm_dropouts = [0.1, 0.2, 0.01]\n",
    "  dense_neurons = [[64, 128], [32, 64], [128, 256]]\n",
    "  dense_activations = [[\"relu\", \"relu\"], [\"tanh\", \"tanh\"], [\"relu\", \"tanh\"]]\n",
    "  model_id = 0\n",
    "\n",
    "  for units in lstm_units:\n",
    "    for dropout in lstm_dropouts:\n",
    "      for neurons in dense_neurons:\n",
    "        for activations in dense_activations:\n",
    "          print(dropout, neurons)\n",
    "          cross_validation(units, dropout, neurons, activations, model_id, get_embeddings)\n",
    "          model_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pCyfSNeaYpu4"
   },
   "outputs": [],
   "source": [
    "def get_baseline_lstm_model(lstm_units=64, lstm_dropout=0.1, num_of_dense=3, dense_neurons=[64,128], dense_activations=[\"relu\", \"relu\"]):\n",
    "  # LSTM Approach\n",
    "  inputA = Input(shape=(60,300))\n",
    "  inputB = Input(shape=(60,300))\n",
    "\n",
    "  # first branch for first input\n",
    "  x = SeqSelfAttention()(inputA)\n",
    "  x = Bidirectional(LSTM(units=lstm_units, return_sequences=False, dropout=lstm_dropout))(x)\n",
    "  # second branch for second input\n",
    "  y = SeqSelfAttention()(inputB)\n",
    "  y = Bidirectional(LSTM(units=lstm_units, return_sequences=False, dropout=lstm_dropout))(y)\n",
    "  # combines the two branches\n",
    "  combined = Concatenate(axis=-1)([x, y])\n",
    "  # FC layers\n",
    "  z = Dense(dense_neurons[0], activation=dense_activations[0])(combined)\n",
    "  for i in range(1, num_of_dense - 1):\n",
    "    z = Dense(dense_neurons[i], activation=dense_activations[i])(z)\n",
    "  z = Dense(1)(z)\n",
    "\n",
    "  model = Model(inputs=[inputA, inputB], outputs=z)\n",
    "  model.summary()\n",
    "  model.compile(\n",
    "      loss='mse',\n",
    "      optimizer='Adam',\n",
    "      metrics=['mae']\n",
    "  )\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZnPKZp3bRS5k"
   },
   "outputs": [],
   "source": [
    "def cross_validation(units, dropout, neurons, activations, model_id, get_embeddings):\n",
    "  kf = KFold(n_splits=8, shuffle=False, random_state=None)\n",
    "\n",
    "  train_f_en = open('./train.ende.src')\n",
    "  lines_train_en = train_f_en.readlines()\n",
    "  train_f_de = open('./train.ende.mt')\n",
    "  lines_train_de = train_f_de.readlines()\n",
    "  val_f_en = open('./dev.ende.src')\n",
    "  lines_val_en = val_f_en.readlines()\n",
    "  val_f_de = open('./dev.ende.mt')\n",
    "  lines_val_de = val_f_de.readlines()\n",
    "  train_scores_f = open('./train.ende.scores')\n",
    "  train_scores = train_scores_f.readlines()\n",
    "  val_scores_f = open('./dev.ende.scores')\n",
    "  val_scores = val_scores_f.readlines()\n",
    "\n",
    "  # Combine training and validation data\n",
    "  data = []\n",
    "  for i in range(len(lines_train_en)):\n",
    "    data.append((lines_train_en[i], lines_train_de[i]))\n",
    "  \n",
    "  for i in range(len(lines_val_en)):\n",
    "    data.append((lines_val_en[i], lines_val_de[i]))\n",
    "\n",
    "  scores = []\n",
    "  for score in train_scores:\n",
    "    scores.append(float(score))\n",
    "  \n",
    "  for score in val_scores:\n",
    "    scores.append(float(score))\n",
    "\n",
    "  shuffle(data, scores, random_state=42)\n",
    "  \n",
    "  average_pearson = 0\n",
    "  average_mse = 0\n",
    "  split = 0\n",
    "  for train_index, val_index in kf.split(np.array(data)):\n",
    "    # Get splits\n",
    "    X_train, y_train = np.array(data)[train_index], np.array(scores)[train_index]\n",
    "    X_val, y_val = np.array(data)[val_index], np.array(scores)[val_index]\n",
    "    X_train = X_train.tolist()\n",
    "    X_val = X_val.tolist()\n",
    "    y_train = y_train.tolist()\n",
    "    y_val = y_val.tolist()\n",
    "    en_train_input = [x for (x, _) in X_train]\n",
    "    de_train_input = [y for (_, y) in X_train]\n",
    "    en_val_input = [x for (x, _) in X_val]\n",
    "    de_val_input = [y for (_, y) in X_val]\n",
    "\n",
    "    # Get embeddings\n",
    "    en_train_input = get_embeddings(en_train_input, nlp_en, stop_words_en, 'en')\n",
    "    de_train_input = get_embeddings(de_train_input, nlp_de, stop_words_de, 'de')\n",
    "    en_train_input = pad_sent(en_train_input)\n",
    "    de_train_input = pad_sent(de_train_input)\n",
    "    en_val_input = get_embeddings(en_val_input, nlp_en, stop_words_en, 'en')\n",
    "    de_val_input = get_embeddings(de_val_input, nlp_de, stop_words_de, 'de')\n",
    "    en_val_input = pad_sent(en_val_input)\n",
    "    de_val_input = pad_sent(de_val_input)\n",
    "\n",
    "    # Keep 500 samples for testing\n",
    "    X_test_en = en_val_input[500:]\n",
    "    X_test_de = de_val_input[500:]\n",
    "    y_test = y_val[500:]\n",
    "    en_val_input = en_val_input[:500]\n",
    "    de_val_input = de_val_input[:500]\n",
    "    y_val = y_val[:500]\n",
    "    \n",
    "    # Train model\n",
    "    model = get_baseline_lstm_model(lstm_units=units, lstm_dropout=dropout, dense_neurons=neurons, dense_activations=activations)\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "    history = model.fit([np.array(en_train_input), np.array(de_train_input)],np.array(y_train), epochs=10, validation_data=([en_val_input, de_val_input], y_val), verbose=1, batch_size=512, callbacks=[es])\n",
    "\n",
    "    # Get metrics for validation predictions\n",
    "    predictions = model.predict([np.array(X_test_en), np.array(X_test_de)])\n",
    "    (pearson, _) = pearsonr(predictions.squeeze(), y_test)\n",
    "    average_pearson += pearson\n",
    "    print(\"Pearson score: \", pearson)\n",
    "    mse, _ = model.evaluate([np.array(X_test_en), np.array(X_test_de)], y_test)\n",
    "    average_mse += mse\n",
    "    print(\"MSE: \", mse)\n",
    "    split += 1\n",
    "  print(\"Average pearson score: \", average_pearson / 8)\n",
    "  print(\"Average mse: \", average_mse / 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 494
    },
    "colab_type": "code",
    "id": "PYooHUjSSet6",
    "outputId": "2afbb78d-816e-4a74-8668-22b8b5e95447"
   },
   "outputs": [],
   "source": [
    "param_tuning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mi4Uzp8pQRLF"
   },
   "source": [
    "## 6. Different embeddings with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "colab_type": "code",
    "id": "tZpqf9RXcE5k",
    "outputId": "5983eba5-6973-4ad4-b3f4-9cee77a1d2ad"
   },
   "outputs": [],
   "source": [
    "# Install FastText\n",
    "!git clone https://github.com/facebookresearch/fastText.git\n",
    "!pip install ./fastText/.\n",
    "import fasttext\n",
    "import fasttext.util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zQpWG2ttQO-9"
   },
   "source": [
    "Embeddings class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ytpiJTNgY6rk"
   },
   "outputs": [],
   "source": [
    "# Embeddings class for FastText and Muse\n",
    "class Embedding:\n",
    "\n",
    "  def __init__(self):\n",
    "    self.ft = None\n",
    "    self.ft_de = None\n",
    "    self.nlp_de = None\n",
    "    self.nlp_en = None\n",
    "    self.wvecs = None\n",
    "    self.german_wvecs = None\n",
    "\n",
    "  def download_fast_text(self):\n",
    "    !wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
    "    !gunzip cc.en.300.bin.gz\n",
    "    !wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.de.300.bin.gz\n",
    "    !gunzip cc.de.300.bin.gz\n",
    "\n",
    "    self.ft = fasttext.load_model('cc.en.300.bin')\n",
    "    self.ft_de = fasttext.load_model('cc.de.300.bin')\n",
    "    fasttext.util.reduce_model(self.ft, 100)\n",
    "    self.ft.save_model('/drive/My Drive/cc.en.100.bin')\n",
    "\n",
    "    fasttext.util.reduce_model(self.ft_de, 100)\n",
    "    self.ft.save_model('drive/My Drive/cc.de.100.bin')\n",
    "\n",
    "  def load_fast_text(self):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    self.ft_en = fasttext.load_model('drive/My Drive/cc.de.100.bin')\n",
    "    self.ft_de = fasttext.load_model('drive/My Drive/cc.en.100.bin')\n",
    "\n",
    "  def load_muse(self):\n",
    "    !wget https://dl.fbaipublicfiles.com/arrival/vectors/wiki.multi.en.vec\n",
    "    !wget https://dl.fbaipublicfiles.com/arrival/vectors/wiki.multi.de.vec\n",
    "\n",
    "    self.wvecs = {}\n",
    "    with open(\"./wiki.multi.en.vec\", \"r\") as ende_src:\n",
    "      for line in ende_src:\n",
    "        word = line.split(\" \")[0]\n",
    "        vector = [float(a) for a in line.split(\" \")[1:]]\n",
    "        self.wvecs[word] = vector\n",
    "\n",
    "    self.german_wvecs = {}\n",
    "    with open(\"./wiki.multi.de.vec\", \"r\") as ende_src:\n",
    "      for line in ende_src:\n",
    "        word = line.split(\" \")[0]\n",
    "        vector = [float(a) for a in line.split(\" \")[1:]]\n",
    "        self.german_wvecs[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6l1Qn3_gVZgh"
   },
   "outputs": [],
   "source": [
    "def get_fasttext_embeddings(lines, nlp, stopwords, lang):\n",
    "  unknown = nlp.vocab['unk'].vector\n",
    "  punctuation = [',','.','...','\\'', '\"', '(', ')', '[', ']']\n",
    "  lines_embs = []\n",
    "  \n",
    "  documents = nlp.pipe(lines, batch_size=32, n_threads=7)\n",
    "  embedding = Embedding()\n",
    "  embedding.load_fast_text()\n",
    "  for doc in documents:\n",
    "    embs = []\n",
    "    for token in doc:\n",
    "      if token.text in stopwords or token.text in punctuation:\n",
    "        continue\n",
    "      if lang == 'en':\n",
    "        embs.append(embedding.ft_en.get_word_vector(token.text))\n",
    "      else:\n",
    "        embs.append(embedding.ft_de.get_word_vector(token.text))\n",
    "    lines_embs.append(embs)\n",
    "  return lines_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5MXZEOB-Qz6u"
   },
   "outputs": [],
   "source": [
    "def get_fasttext_embedding_results():\n",
    "  # Get best model\n",
    "  best_model = get_baseline_lstm_model(lstm_units=64, lstm_dropout=0.1, num_of_dense=3, dense_neurons=[64,128], dense_activations=[\"relu\", \"relu\"])\n",
    "\n",
    "  # Cross validation\n",
    "  cross_validation(64, 0.1, [64,128],['relu','relu'], 0, None, get_fasttext_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "gcyzzjTRayLr",
    "outputId": "66afebf4-db35-418b-edaf-8ff8e2d74f86"
   },
   "outputs": [],
   "source": [
    "get_fasttext_embedding_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "XkqBmWNsypaD",
    "outputId": "2851ad89-9d99-47b8-d31f-3329529a8b4a"
   },
   "outputs": [],
   "source": [
    "!pip install bert-embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WOzSGdsxy4YR"
   },
   "outputs": [],
   "source": [
    "from bert_embedding import BertEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nZ8KxH3EVchI"
   },
   "outputs": [],
   "source": [
    "def get_bert_embeddings(lines, nlp, stopwords, lang):\n",
    "  unknown = nlp.vocab['unk'].vector\n",
    "  punctuation = [',','.','...','\\'', '\"', '(', ')', '[', ']']\n",
    "  lines_embs = []\n",
    "\n",
    "  documents = nlp.pipe(lines, batch_size=32, n_threads=7)\n",
    "  embedding = BertEmbedding(model='bert_12_768_12', dataset_name='wiki_multilingual')\n",
    "  for doc in documents:\n",
    "    l = []\n",
    "    embs = []\n",
    "    for token in doc:\n",
    "      if token.text in stopwords or token.text in punctuation:\n",
    "        continue\n",
    "      l.append(token.text)\n",
    "    lines_embs.append(l)\n",
    "  \n",
    "  new_lines = []\n",
    "  for line in lines_embs:\n",
    "    line = \" \".join([w for w in line])\n",
    "    new_lines.append(line)\n",
    "\n",
    "  bert_res = embedding(new_lines)\n",
    "  res = [emb for (_,emb) in bert_res]\n",
    "  return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DI4Ki4kXRRWG"
   },
   "outputs": [],
   "source": [
    "def get_bert_embedding_results():\n",
    "  # Get best model\n",
    "  best_model = get_baseline_lstm_model(lstm_units=64, lstm_dropout=0.1, num_of_dense=3, dense_neurons=[64,128], dense_activations=[\"relu\", \"relu\"])\n",
    "\n",
    "  # Cross validation\n",
    "  cross_validation(64,0.1,[64,128],['relu','relu'], 0, None, get_bert_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "M-4uaZR2z30n",
    "outputId": "f549bba0-2a5a-4eb7-f778-b731163eb759"
   },
   "outputs": [],
   "source": [
    "get_bert_embedding_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jc30IDF6jVBN"
   },
   "source": [
    "## 7. Best LSTM model training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "id": "qNl6EGuMjctv",
    "outputId": "4cc3df43-d8a3-4eea-e16c-0abd4dc179c6"
   },
   "outputs": [],
   "source": [
    "# Get training and validation embeddings\n",
    "lines_en = open('./train.ende.src').readlines()\n",
    "lines_de = open('./train.ende.mt').readlines()\n",
    "\n",
    "english_train_embeddings = get_embeddings(lines_en, nlp_en, stop_words_en, 'en')\n",
    "german_train_embeddings = get_embeddings(lines_de, nlp_de, stop_words_de, 'de')\n",
    "\n",
    "scores_train = get_scores('./train.ende.scores')\n",
    "\n",
    "english_val_embeddings = get_embeddings(open('./dev.ende.src').readlines(), nlp_en, stop_words_en, 'en')\n",
    "german_val_embeddings = get_embeddings(open('./dev.ende.mt').readlines(), nlp_de, stop_words_de, 'de')\n",
    "\n",
    "scores_val = get_scores('./dev.ende.scores')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "iTo6EScf0NV1",
    "outputId": "0abcb37e-d108-472c-871b-176702606bd9"
   },
   "outputs": [],
   "source": [
    "best_model = get_baseline_lstm_model(lstm_units=64, lstm_dropout=0.1, num_of_dense=3, dense_neurons=[64,128], dense_activations=[\"relu\", \"relu\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MCPqt-AKnZND"
   },
   "outputs": [],
   "source": [
    "english_train_embeddings = pad_sent(english_train_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "09oSpVlfngFK"
   },
   "outputs": [],
   "source": [
    "german_train_embeddings = pad_sent(german_train_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cgbN4rXRoFHd"
   },
   "outputs": [],
   "source": [
    "english_val_embeddings = pad_sent(english_val_embeddings)\n",
    "german_val_embeddings = pad_sent(german_val_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429
    },
    "colab_type": "code",
    "id": "lSm8x8HHjZBY",
    "outputId": "c07ae4e2-9246-4366-cd5b-3c86a5ee60f9"
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "history = best_model.fit([np.array(english_train_embeddings), np.array(german_train_embeddings)],np.array(scores_train), epochs=10, validation_data=([english_val_embeddings[:500], german_val_embeddings[:500]], scores_val[:500]), verbose=1, batch_size=1024, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_lvLO4trpdbm",
    "outputId": "49e25990-b61c-4e59-ab4f-9812ae79bbb2"
   },
   "outputs": [],
   "source": [
    "pearsonr(best_model.predict([np.array(english_val_embeddings[500:]), np.array(german_val_embeddings[500:])]).squeeze(), np.array(scores_val[500:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "vvGn_HbJXQkM",
    "outputId": "be9ef46f-08c2-4224-9bcf-5190fcb1aab6"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "# plt.show()\n",
    "plt.savefig('loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LHvGTcSGozwW"
   },
   "outputs": [],
   "source": [
    "# Get test embeddings\n",
    "english_test_embeddings = get_embeddings(open('./test.ende.src').readlines(), nlp_en, stop_words_en, 'en')\n",
    "english_test_embeddings = pad_sent(english_test_embeddings)\n",
    "german_test_embeddings = get_embeddings(open('./test.ende.mt').readlines(), nlp_de, stop_words_de, 'de')\n",
    "german_test_embeddings = pad_sent(german_test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SU30fRZSl7gu"
   },
   "outputs": [],
   "source": [
    "predictions_test = model.predict([np.array(english_test_embeddings), np.array(german_test_embeddings)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h4OygMp-pcRR"
   },
   "outputs": [],
   "source": [
    "f = open(\"predictions.txt\", \"w\")\n",
    "for num in predictions_test:\n",
    "  f.write(f\"{num[0]}\\n\")\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Embeddings models.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
